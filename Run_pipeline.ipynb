{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from rasterio.mask import mask\n",
    "from os.path import join\n",
    "import descartes\n",
    "from PIL import Image\n",
    "\n",
    "import fiona \n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "# see: https://github.com/karolzak/keras-unet\n",
    "from keras_unet.models import satellite_unet \n",
    "from keras_unet.losses import jaccard_distance\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.utils import get_augmented\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data as ld\n",
    "import preprocessing as pp\n",
    "import prepare_for_network as pfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths, set your own path here\n",
    "path_data_main = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge'\n",
    "\n",
    "# put your data in these dirs\n",
    "path_data_local_train = join(path_data_main, 'Train_Dataset')\n",
    "path_data_local_val = join(path_data_main, 'Validation_Dataset')\n",
    "path_data_local_test = join(path_data_main, 'Evaluation_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dirs are made automatically\n",
    "dir_temp_data =  ld.make_dir(join(path_data_main, 'data_temp'))\n",
    "dir_temp_data_train = ld.make_dir(join(dir_temp_data, 'train'))\n",
    "dir_temp_data_test = ld.make_dir(join(dir_temp_data, 'test'))\n",
    "dir_temp_plots = ld.make_dir(join(dir_temp_data, 'plots'))\n",
    "dir_temp_labels = ld.make_dir(join(dir_temp_data, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "#size_sub_sample = (256, 256)\n",
    "size_sub_sample = (512, 512)\n",
    "inv_stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files in 'Train_Dataset' are automatically selected for training.\n",
    "All files in 'Validation_Dataset' are automatically selected for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings preprocessing\n",
    "quantile_clip_max = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Mosul\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Najaf\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Nasiryah\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Souleimaniye\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_label_data() missing 1 required positional argument: 'dir_save_plots'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a7a4c7a4a49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict_raster_layers_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_paths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_temp_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantile_clip_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_paths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_raster_layers_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlist_paths_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_stacked_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_temp_data_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_label_data() missing 1 required positional argument: 'dir_save_plots'"
     ]
    }
   ],
   "source": [
    "dict_paths_train = ld.paths_in_dict(path_data_local_train)\n",
    "\n",
    "dict_raster_layers_train, dict_data_train = pp.preprocess_data(dict_paths_train, dir_temp_plots, quantile_clip_max)\n",
    "dict_labels = pp.get_label_data(dict_paths_train, dict_raster_layers_train)\n",
    "\n",
    "list_paths_data_train = pp.save_stacked_arrays(dict_data_train, dir_temp_data_train)\n",
    "list_paths_labels = pp.save_labels(dict_labels, dir_temp_labels)\n",
    "\n",
    "\n",
    "#_, dict_data_test = pp.preprocess_data(dict_info_data_train, dir_temp_plots, quantile_clip_max)\n",
    "#list_paths_data_test = pp.save_stacked_arrays(dict_data_test, dir_temp_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_info_data_val  = ld.paths_in_dict(dir_temp_data_val)\n",
    "\n",
    "dict_raster_layers_val, dict_data_val = pp.preprocess_data(dict_info_data_val, dir_temp_plots, quantile_clip_max)\n",
    "dict_labels_val = pp.get_label_data(dict_data, dict_raster_layers_train, dir_temp_labels)\n",
    "\n",
    "list_paths_data_val = pp.save_stacked_arrays(dict_data_val, dir_temp_data_val)\n",
    "list_paths_labels_val = pp.save_labels(dict_labels, dir_temp_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_data, list_paths_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## prepare data for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul.npy Mosul.npy\n",
      "(8, 10980, 10979)\n",
      "new size padding 1: (8, 11264, 11264)\n",
      "padding 2:\n",
      " dim_str: \n",
      " new size array: (8, 11264, 11264)\n",
      " shape after split: (484, 8, 512, 512)\n",
      "save_array_in_new_sub_dir\n",
      "padding 2:\n",
      " dim_str: x\n",
      "x1: (8, 11264, 11264)\n",
      "x2: (8, 11776, 11264)\n",
      " new size array: (8, 11776, 11264)\n",
      "x shape after split: (506, 8, 512, 512)\n",
      "save_array_in_new_sub_dir\n",
      "padding 2:\n",
      " dim_str: y\n",
      " new size array: (8, 11264, 11776)\n",
      "y shape after split: (506, 8, 512, 512)\n",
      "save_array_in_new_sub_dir\n",
      "padding 2:\n",
      " dim_str: xy\n",
      "x1: (8, 11264, 11264)\n",
      "x2: (8, 11776, 11264)\n"
     ]
    }
   ],
   "source": [
    "dir_temp_data_train_whole = ld.make_dir(join(dir_temp_data_train, 'whole'))\n",
    "dir_temp_labels_whole = ld.make_dir(join(dir_temp_labels, 'whole'))\n",
    "\n",
    "list_dir_np = [f for f in os.listdir(dir_temp_data_train_whole) if os.path.splitext(f)[1] == '.npy']\n",
    "list_dir_np_label = [f for f in os.listdir(dir_temp_labels_whole) if os.path.splitext(f)[1] == '.npy']\n",
    "\n",
    "for area_data_file, area_label_file in zip(list_dir_np, list_dir_np_label):\n",
    "    print(area_data_file, area_label_file)\n",
    "    pipeline_prepare_for_nn(area_data_file, dir_temp_data_train, inv_stride, size_sub_sample)\n",
    "    pipeline_prepare_for_nn_labels(area_label_file, dir_temp_labels, inv_stride, size_sub_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size_sub_sample = (256, 256)\n",
    "quantile_clip_max = 0.999\n",
    "size_sub_sample = (512, 512)\n",
    "inv_stride = 2\n",
    "EPOCHS = 1\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "IMAGE_DIMS =  size_sub_sample + (8,)\n",
    "model = satellite_unet(IMAGE_DIMS)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('load data...')\n",
    "if test_run:\n",
    "    # testing with one area\n",
    "    trainX = np.load(file_X)\n",
    "    trainY = np.load(file_Y)\n",
    "else:\n",
    "    # for real with all areas, big data\n",
    "    trainX = ld.load_entire_dataset(join(dir_temp_data_train, 'split'))\n",
    "    trainY = ld.load_entire_dataset(join(dir_temp_labels, 'split'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[2], trainX.shape[3], trainX.shape[1])\n",
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], trainY.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = get_augmented(\n",
    "        trainX,\n",
    "        trainY,\n",
    "        X_val=None,\n",
    "        Y_val=None,\n",
    "        batch_size=BS,\n",
    "        seed=0,\n",
    "        data_gen_args=dict(\n",
    "            rotation_range=90,\n",
    "            height_shift_range=0,\n",
    "            shear_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =\"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit_generator(generator=train_datagen,\n",
    "                        steps_per_epoch=len(trainX) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1015021568 into shape (8,512,484)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5a9170367f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1015021568 into shape (8,512,484)"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_unosatenv]",
   "language": "python",
   "name": "conda-env-conda_unosatenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
