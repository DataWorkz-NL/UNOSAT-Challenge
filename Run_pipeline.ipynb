{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save np.arrays as h5 files instead of .npy\n",
    "# TODO: generator input deep learning\n",
    "# TODO: post processing: reconstruct image from network output and create shape files\n",
    "# TODO: contruct my own f1 score definition, perhaps take iou=0.5 per polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install pandas\n",
    "    !pip install matplotlib\n",
    "    !pip install rasterio\n",
    "    !pip install shapely\n",
    "    !pip install descartes\n",
    "    !pip install pyproj\n",
    "    !pip install geopandas\n",
    "    !pip install keras_unet\n",
    "    !pip install h5py\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    js = ('<script>function ConnectButton(){ '\n",
    "            'console.log(\"Connect pushed\"); '\n",
    "            'document.querySelector(\"#connect\").click()} '\n",
    "            'setInterval(ConnectButton,3000);</script>')\n",
    "    display(HTML(js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from rasterio.mask import mask\n",
    "from os.path import join\n",
    "import descartes\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import fiona \n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras import callbacks as keras_cb \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# see: https://github.com/karolzak/keras-unet\n",
    "from keras_unet.models import satellite_unet \n",
    "from keras_unet.losses import jaccard_distance\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.utils import get_augmented\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# # prevent Tensorflow memory leakage\n",
    "# K.clear_session()\n",
    "\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "print(sys.executable)\n",
    "\n",
    "if colab:\n",
    "    sys.path.append(\"/content/drive/My Drive/app_UNOSAT\")\n",
    "    #sys.path.append(\"/content/drive/My Drive/app_UNOSAT/src\")\n",
    "\n",
    "from src import load_data as ld\n",
    "from src import preprocessing as pp\n",
    "from src import prepare_for_network as pfn\n",
    "from src import utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(pfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths, set your own path here\n",
    "if colab:\n",
    "    #path_main = '/content/drive/My Drive/app_UNOSAT'\n",
    "    path_main = '/content/drive/Computers/My Macbook Pro'\n",
    "else:\n",
    "    path_main = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge'\n",
    "\n",
    "path_data_main = utl.make_dir(join(path_main, \"data\"))\n",
    "\n",
    "#### put data in these dirs\n",
    "\n",
    "# I choose:\n",
    "# Training: Mosul, Najaf, Nasiryah\n",
    "# Validation: Souleimaniye\n",
    "# Test: Bagdad, Kirkouk, Samawah, Tikrit (no labels present)\n",
    "path_data_local_train = join(path_data_main, 'Train_Dataset')\n",
    "path_data_local_val = join(path_data_main, 'Validation_Dataset')\n",
    "path_data_local_test = join(path_data_main, 'Evaluation_Dataset')\n",
    "\n",
    "path_model =  utl.make_dir(join(path_data_main, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dirs are made automatically\n",
    "dir_temp_data =  utl.make_dir(join(path_data_main, 'data_temp'))\n",
    "dir_temp_train = utl.make_dir(join(dir_temp_data, 'train'))\n",
    "dir_temp_val = utl.make_dir(join(dir_temp_data, 'val'))\n",
    "dir_temp_dev = utl.make_dir(join(dir_temp_data, 'data_dev'))\n",
    "dir_temp_eval = utl.make_dir(join(dir_temp_data, 'evaluation'))\n",
    "dir_temp_plots = utl.make_dir(join(dir_temp_data, 'plots_preprocessing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings preprocessing\n",
    "quantile_clip_max = 0.999\n",
    "size_sub_sample = (512, 512)\n",
    "dtype = 'i16'\n",
    "inv_stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All files in 'Train_Dataset' are automatically selected for training (saved in sub dir 'whole'). All files in 'Validation_Dataset' are automatically selected for validation. All files in 'Evaluation_Dataset' are automatically selected for testing.<br>\n",
    "Data (.tif files) and labels (.shp files) are both converted to .hdf5 files. First preprocessing is done for the .tif files (clip at quantile 'quantile_clip_max'). The label files become mask arrays, with 1=urban area and 0=non-urban area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pad_zeros_xy_single_side(arr, size_sub_sample):\n",
    "    shape_x, shape_y = arr.shape\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    print(\"arr.shape:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "    \n",
    "    #print('new shape:', arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dtype = 'i8'\n",
    "#dtype = 'i16'\n",
    "sub_sample_shape = (512, 512)\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "file_hfd5 = join(dir_temp_dev,  'testrun_X.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Najaf\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Nasiryah\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with hdf5\n",
    "dtype = 'i8'\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "path_file_hfd5 = join(dir_temp_train,  'groups_test_X.hdf5')\n",
    "\n",
    "with h5py.File(file_hfd5, 'w') as f:\n",
    "    for area in dict_paths:\n",
    "        print(area)\n",
    "        i = 0\n",
    "        #g = f.create_group(area)\n",
    "        dir_save_plots = utl.make_dir(join(dir_temp_plots, area))        \n",
    "        \n",
    "        for season in ld.seasons_fixed_order:\n",
    "            for pol in sorted(dict_paths[area]['tif'][season]):\n",
    "                print(' ', season, pol)\n",
    "                path_raster = dict_paths[area]['tif'][season][pol]\n",
    "                np_arr_processed = pp.process_image(rio.open(path_raster).read()[0], \n",
    "                                                    quantile_clip_max,\n",
    "                                                    clip_min=0, \n",
    "                                                    plotting=True, \n",
    "                                                    dir_save_plots=dir_save_plots,\n",
    "                                                    area=area,  season=season, pol=pol)\n",
    "\n",
    "                shape_x, shape_y = np_arr_processed.shape\n",
    "                dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "                dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "                \n",
    "                #np_arr_processed = pad_zeros_xy(np_arr_processed, dim_x_add, dim_y_add)\n",
    "                #np_arr_shape_new = np_arr_processed.shape + (1,)\n",
    "                stacked_shape = (shape_x + dim_x_add, shape_y + dim_y_add)  + (8,)        \n",
    "                \n",
    "                if i == 0:\n",
    "                    dset = f.create_dataset(area, \n",
    "                                            stacked_shape,\n",
    "                                            dtype=dtype,\n",
    "                                            chunks=True)\n",
    "                    print(' shape init', dset.shape)\n",
    "                    \n",
    "                dset[:, :, i] = pad_zeros_xy(np_arr_processed, dim_x_add, dim_y_add)\n",
    "                i += 1\n",
    "                \n",
    "        print(\"shape dataset:\", dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_file_hfd5, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hfd5_labels = join(dir_temp_dev, 'testrun_Y.hdf5')\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 4316046\n",
      "Najaf\n",
      "polygon mask shape: (10980, 10980)\n",
      "sum polygon_mask_int: 3709703\n",
      "Nasiryah\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 1228702\n"
     ]
    }
   ],
   "source": [
    "def shp2polygons(dict_paths):\n",
    "    dict_polygons = dict()   \n",
    "    for area in dict_paths:\n",
    "        path_shape = dict_paths[area]['shp']\n",
    "        dict_polygons[area] = gpd.read_file(path_shape)\n",
    "    return dict_polygons\n",
    "                \n",
    "def preprocess_shape(dict_paths, file_hfd5_labels, sub_sample_shape):\n",
    "    with h5py.File(file_hfd5_labels, 'w') as f:\n",
    "        for area in dict_paths:\n",
    "            print(area)\n",
    "\n",
    "            dict_polygons = shp2polygons(dict_paths)\n",
    "\n",
    "            # raster obj only required for shape and crs, this is season or pol independent\n",
    "            path_raster = dict_paths[area]['tif']['spring']['vh']\n",
    "            raster_obj = rio.open(path_raster)\n",
    "            arr_shape = raster_obj.shape\n",
    "\n",
    "            dim_x_add = sub_sample_shape[0] - arr_shape[0] % sub_sample_shape[0]\n",
    "            dim_y_add = sub_sample_shape[1] - arr_shape[1] % sub_sample_shape[1]       \n",
    "            arr_new_shape = (arr_shape[0] + dim_x_add,  arr_shape[1] + dim_y_add)\n",
    "\n",
    "            dset = f.create_dataset(area, \n",
    "                                    arr_new_shape,\n",
    "                                    dtype=dtype,\n",
    "                                    chunks=True)\n",
    "\n",
    "            dset[:] = pad_zeros_xy(pp.polygons_2_np_arr_mask(dict_polygons[area], \n",
    "                                                                 raster_obj),\n",
    "                                   dim_x_add,\n",
    "                                   dim_y_add\n",
    "                                  )       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Mosul\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Najaf\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Nasiryah\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 4316046\n",
      "polygon mask shape: (10980, 10980)\n",
      "sum polygon_mask_int: 3709703\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 1228702\n",
      "stack and save arrays...\n",
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n",
      "save labels...\n",
      "takes 5 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_train = ld.paths_in_dict(path_data_local_train)\n",
    "\n",
    "file_hfd5_train_X = join(dir_temp_train, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_train_Y = join(dir_temp_train, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_train, file_hfd5_train_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_train, file_hfd5_train_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Souleimaniye\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10979, 10980)\n",
      "sum polygon_mask_int: 1861770\n",
      "stack and save arrays...\n",
      "Souleimaniye\n",
      "save labels...\n",
      "takes 2 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_val = ld.paths_in_dict(path_data_local_val)\n",
    "\n",
    "file_hfd5_val_X = join(dir_temp_val, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_val_Y = join(dir_temp_val, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_val, file_hfd5_val_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_val, file_hfd5_val_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_test = ld.paths_in_dict(path_data_local_test)\n",
    "\n",
    "file_hfd5_test_X = join(dir_temp_test, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_test_Y = join(dir_temp_test, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_test, file_hfd5_test_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_test, file_hfd5_test_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## prepare data for network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Images and labels are in .npy / .h5 file shape now, but they need to be converted to suitable input for a network.\n",
    "Therefore a full image is cut into many sub-samples of size 'size_sub_sample', e.g. (512, 512). This requires first zero padding (func zero_padding_1) in order to fit the image to an integer times size_sub_sample. <br>\n",
    "Then we augment data by using a stride of size_sub_sample/inv_stride, i.e. we shift the image by for example half a sample size (inv_stride=2). Subsequently we cut into sub-images.  This results in inv_stride<sup>2</sup> times more data. In order to fit n sub-samples into a shifted image, new padding is required (func zero_padding_2). <br>\n",
    "Output is saved in sub dir 'split'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_all(name):\n",
    "#     print(name)\n",
    "\n",
    "# def get_shape(ds):\n",
    "#     print(ds.shape)\n",
    "    \n",
    "# with h5py.File(path_hd5, 'r') as f:\n",
    "#     for key in f.keys():\n",
    "#         ds = f[key]\n",
    "#         print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    #print('padding 2:')\n",
    "    #print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    #print(\" new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices of shape (nrows, ncols)\"\"\"\n",
    "    r, h = array.shape\n",
    "    return (array.reshape(h//nrows, nrows, -1, ncols)\n",
    "                 .swapaxes(1, 2)\n",
    "                 .reshape(-1, nrows, ncols))\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])\n",
    "\n",
    "def list_stride(mode):\n",
    "    if mode == 0:\n",
    "        return ['_']\n",
    "    if mode == 1:\n",
    "        return ['_', 'x', 'y', 'xy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date => 1576842805.787124\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_arr = np.array([1,1])\n",
    "\n",
    "with h5py.File('test.hdf5', 'w') as f:\n",
    "    g = f.create_group('Base_Group')\n",
    "    d = g.create_dataset('default', data=test_arr)\n",
    "\n",
    "    metadata = {'Date': time.time()}\n",
    "\n",
    "    f.attrs.update(metadata)\n",
    "\n",
    "    for m in f.attrs.keys():\n",
    "        print('{} => {}'.format(m, f.attrs[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img=8):\n",
    "    t0 = time.time()\n",
    "    with h5py.File(file_split_hfd5, 'w') as f2:\n",
    "        with h5py.File(file_hfd5, 'r') as f1:\n",
    "\n",
    "            idx_i = 0\n",
    "            meta_data = dict()\n",
    "\n",
    "            for area in sorted(f1.keys()):\n",
    "                print(area)\n",
    "                dset = f1[area]\n",
    "                print(dset.shape)\n",
    "\n",
    "                for dim_str in list_stride(stride_mode):\n",
    "                    print(\"dim_str:\", dim_str)\n",
    "                    np_arr_shape = list(dset.shape)\n",
    "                    if 'x' in dim_str:\n",
    "                        np_arr_shape[0] = np_arr_shape[0] + size_sub_sample[0]\n",
    "                    if 'y' in dim_str:\n",
    "                        np_arr_shape[1] = np_arr_shape[1] + size_sub_sample[1]\n",
    "\n",
    "                    idx_j = int(np_arr_shape[0] * np_arr_shape[1] / (size_sub_sample[0] * size_sub_sample[1])) + idx_i\n",
    "\n",
    "                    if dataset_kind == 'labels':\n",
    "                        np_arr_shape_split = (idx_j,) + size_sub_sample\n",
    "                        maxshape = (None,) + size_sub_sample\n",
    "                    elif dataset_kind == 'data':\n",
    "                        np_arr_shape_split = (idx_j,) + size_sub_sample + (depth_img,)\n",
    "                        maxshape = (None,) + size_sub_sample + (depth_img,)\n",
    "\n",
    "                    if idx_i == 0:\n",
    "                        dset2 = f2.create_dataset(name_dset, \n",
    "                                                    np_arr_shape_split,\n",
    "                                                    dtype=dtype,\n",
    "                                                    maxshape=maxshape,\n",
    "                                                    chunks=True)\n",
    "\n",
    "                    if dataset_kind == 'labels':\n",
    "                        dset2.resize((idx_j,) + size_sub_sample)\n",
    "                        dset2[idx_i:idx_j,:,:] = split_array_2D(zero_padding_2(dset[:], \n",
    "                                                                           dim_str,\n",
    "                                                                           inv_stride, \n",
    "                                                                           size_sub_sample), \n",
    "                                                                size_sub_sample[0], \n",
    "                                                                size_sub_sample[1])\n",
    "                    elif dataset_kind == 'data':\n",
    "                        for k in range(depth_img):\n",
    "                            dset2.resize((idx_j,) + size_sub_sample + (depth_img,))\n",
    "                            dset2[idx_i:idx_j,:,:,k] = split_array_2D(zero_padding_2(dset[:,:,k], \n",
    "                                                                           dim_str,\n",
    "                                                                           inv_stride, \n",
    "                                                                           size_sub_sample), \n",
    "                                                                        size_sub_sample[0], \n",
    "                                                                        size_sub_sample[1])\n",
    "                    print(\"dset2.shape:\", dset2.shape)\n",
    "                    print('idx i,j:', idx_i, idx_j)\n",
    "                    meta_data[area + ' ' + dim_str] = (idx_i, idx_j)\n",
    "                    idx_i = idx_j\n",
    "\n",
    "                f2.attrs.update(meta_data)\n",
    "\n",
    "    print('takes {} mins'.format(round((time.time() - t0) / 60)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (484, 512, 512, 8)\n",
      "idx i,j: 0 484\n",
      "dim_str: x\n",
      "dset2.shape: (990, 512, 512, 8)\n",
      "idx i,j: 484 990\n",
      "dim_str: y\n",
      "dset2.shape: (1496, 512, 512, 8)\n",
      "idx i,j: 990 1496\n",
      "dim_str: xy\n",
      "dset2.shape: (2025, 512, 512, 8)\n",
      "idx i,j: 1496 2025\n",
      "Najaf\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (2509, 512, 512, 8)\n",
      "idx i,j: 2025 2509\n",
      "dim_str: x\n",
      "dset2.shape: (3015, 512, 512, 8)\n",
      "idx i,j: 2509 3015\n",
      "dim_str: y\n",
      "dset2.shape: (3521, 512, 512, 8)\n",
      "idx i,j: 3015 3521\n",
      "dim_str: xy\n",
      "dset2.shape: (4050, 512, 512, 8)\n",
      "idx i,j: 3521 4050\n",
      "Nasiryah\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (4534, 512, 512, 8)\n",
      "idx i,j: 4050 4534\n",
      "dim_str: x\n",
      "dset2.shape: (5040, 512, 512, 8)\n",
      "idx i,j: 4534 5040\n",
      "dim_str: y\n",
      "dset2.shape: (5546, 512, 512, 8)\n",
      "idx i,j: 5040 5546\n",
      "dim_str: xy\n",
      "dset2.shape: (6075, 512, 512, 8)\n",
      "idx i,j: 5546 6075\n",
      "takes 72 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "\n",
    "prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (484, 512, 512)\n",
      "idx i,j: 0 484\n",
      "dim_str: x\n",
      "dset2.shape: (990, 512, 512)\n",
      "idx i,j: 484 990\n",
      "dim_str: y\n",
      "dset2.shape: (1496, 512, 512)\n",
      "idx i,j: 990 1496\n",
      "dim_str: xy\n",
      "dset2.shape: (2025, 512, 512)\n",
      "idx i,j: 1496 2025\n",
      "Najaf\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (2509, 512, 512)\n",
      "idx i,j: 2025 2509\n",
      "dim_str: x\n",
      "dset2.shape: (3015, 512, 512)\n",
      "idx i,j: 2509 3015\n",
      "dim_str: y\n",
      "dset2.shape: (3521, 512, 512)\n",
      "idx i,j: 3015 3521\n",
      "dim_str: xy\n",
      "dset2.shape: (4050, 512, 512)\n",
      "idx i,j: 3521 4050\n",
      "Nasiryah\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (4534, 512, 512)\n",
      "idx i,j: 4050 4534\n",
      "dim_str: x\n",
      "dset2.shape: (5040, 512, 512)\n",
      "idx i,j: 4534 5040\n",
      "dim_str: y\n",
      "dset2.shape: (5546, 512, 512)\n",
      "idx i,j: 5040 5546\n",
      "dim_str: xy\n",
      "dset2.shape: (6075, 512, 512)\n",
      "idx i,j: 5546 6075\n",
      "takes 3 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# file_hfd5_labels_split =  join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "\n",
    "# name_dset = 'all_train_data_Y'\n",
    "\n",
    "# with h5py.File(file_hfd5_labels_split, 'w') as f2:\n",
    "#     with h5py.File(file_hfd5_labels, 'r') as f1:\n",
    "#         idx_i = 0\n",
    "#         meta_data = dict()\n",
    "        \n",
    "#         for area in f1.keys():\n",
    "#             print(area)\n",
    "#             dset = f1[area]\n",
    "#             print(dset.shape)\n",
    "\n",
    "#             for dim_str in ['_', 'x', 'y', 'xy']:\n",
    "#                 print(\"dim_str:\", dim_str)\n",
    "#                 np_arr_shape = list(dset.shape)\n",
    "#                 if 'x' in dim_str:\n",
    "#                     np_arr_shape[0] = np_arr_shape[0] + size_sub_sample[0]\n",
    "#                 if 'y' in dim_str:\n",
    "#                     np_arr_shape[1] = np_arr_shape[1] + size_sub_sample[1]\n",
    "                \n",
    "#                 idx_j = int(np_arr_shape[0] * np_arr_shape[1] / (size_sub_sample[0] * size_sub_sample[1])) + idx_i\n",
    "#                 np_arr_shape_split = (idx_j,) + size_sub_sample\n",
    "                \n",
    "#                 if idx_i == 0:\n",
    "#                     dset2 = f2.create_dataset(name_dset, \n",
    "#                                                 np_arr_shape_split,\n",
    "#                                                 dtype=dtype,\n",
    "#                                                 chunks=True)\n",
    "                \n",
    "#                 print(\"dset2.shape:\", dset2.shape)\n",
    "\n",
    "#                 dset2[idx_i:idx_j,:,:] = split_array_2D(zero_padding_2(dset[:], \n",
    "#                                                                        dim_str,\n",
    "#                                                                        inv_stride, \n",
    "#                                                                        size_sub_sample), \n",
    "#                                                         size_sub_sample[0], \n",
    "#                                                         size_sub_sample[1])\n",
    "                \n",
    "                \n",
    "#                 print('idx i,j:', idx_i, idx_j)\n",
    "#                 meta_data[area + ' ' + dim_str] = (idx_i, idx_j)\n",
    "#                 idx_i = idx_j\n",
    "\n",
    "#             f2.attrs.update(meta_data)\n",
    "#             print(\"shape dataset:\", dset2.shape)\n",
    "\n",
    "# print(\"total time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "#TODO: in a different separate step, concatenate datasets of areas\n",
    "# see: https://stackoverflow.com/questions/43929420/how-to-concatenate-two-numpy-arrays-in-hdf5-format\n",
    "\n",
    "# same for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir_label_dev = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/data_temp/train/labels'\n",
    "area_file_label_dev = 'Mosul_label.npy'\n",
    "arr_label_dev = np.load(join(dir_label_dev, 'whole', area_file_label_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-696fb436eb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print(arr.shape)\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fc3d05e335f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdim_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_y_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_y\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_y_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new size padding 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cc25739f4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minv_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print('x1:', arr.shape)\n",
    "dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "print('x2:', arr.shape)\n",
    "\n",
    "dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "sys.getsizeof(arr) / 10**9, arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def zero_padding_1(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print(arr.shape)\n",
    "    dim_x = arr.shape[1]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "    dim_y = arr.shape[2]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "    print(\"new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "    \n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('padding 2:')\n",
    "    print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            print('x1:', arr.shape)\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "            print('x2:', arr.shape)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "    print(\" new size array:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])\n",
    "\n",
    "def save_array_in_new_sub_dir(arr, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('save_array_in_new_sub_dir')\n",
    "    name_new_dir = 'split'\n",
    "    dir_data_split = join(main_dir, name_new_dir) \n",
    "    ld.make_dir(dir_data_split)\n",
    "    path_file_split = join(dir_data_split, os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str)\n",
    "    np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices and .\"\"\"\n",
    "    r, h = array.shape\n",
    "    arr_3D = array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
    "    return [arr for arr in arr_3D]\n",
    "\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                             .swapaxes(1, 2)\n",
    "                             .reshape(-1, nrows, ncols))\n",
    "    \n",
    "        arr_4D = np.array(list_3D_arrs)\n",
    "        \n",
    "    arr_4D = arr_4D.reshape(arr_4D.shape[1], arr_4D.shape[0], arr_4D.shape[2], arr_4D.shape[3])\n",
    "    return [arr for arr in arr_4D]\n",
    "\n",
    "def save_samples(list_arrs, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('  save samples')\n",
    "    dir_data_split = join(main_dir, name_new_dir)\n",
    "    ld.make_dir(dir_data_split)\n",
    "\n",
    "    str_filename =  os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str + '_{}' \n",
    "    for i, arr in enumerate(list_arrs):\n",
    "        path_file_split = join(dir_data_split, str_filename.format(i))\n",
    "        np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "def zero_padding_1_labels(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print('  padding 1')\n",
    "    dim_x = arr.shape[0]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "\n",
    "    dim_y = arr.shape[1]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "    print(\"   new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def zero_padding_2_labels(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('  padding 2:')\n",
    "    print('    dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    print(\"   new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new size padding 1: (8, 11264, 11264)\n"
     ]
    }
   ],
   "source": [
    "# zero padding 1\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding 2:\n",
      " dim_str: \n",
      " new size array: (8, 11264, 11264)\n",
      "pad 2 4.0600864 (8, 11264, 11264)\n",
      "len list: 484\n",
      "shape arr: (8, 512, 512)\n",
      "  save samples\n",
      "padding 2:\n",
      " dim_str: x\n",
      "x1: (8, 11264, 11264)\n",
      "x2: (8, 11776, 11264)\n",
      " new size array: (8, 11776, 11264)\n",
      "pad 2 4.244635776 (8, 11776, 11264)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e17f249c2db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlist_arrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_array_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_area_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'len list:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape arr:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a2e7486f08b8>\u001b[0m in \u001b[0;36msplit_array_3D\u001b[0;34m(array, nrows, ncols)\u001b[0m\n\u001b[1;32m     16\u001b[0m         list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n\u001b[1;32m     17\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                              .reshape(-1, nrows, ncols))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0marr_4D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_3D_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rest of pipeline\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    #zero padding 2\n",
    "    arr_area_2 = zero_padding_2(arr, dim_str, inv_stride, size_sub_sample)\n",
    "    print('pad 2',sys.getsizeof(arr_area_2) / 10**9, arr_area_2.shape)\n",
    "\n",
    "    #split\n",
    "    list_arrs = split_array_3D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    print('len list:', len(list_arrs))\n",
    "    print('shape arr:', list_arrs[0].shape)\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    \n",
    "    save_samples(list_arrs, dir_data_dev, 'split', area_file_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "    \n",
    "del arr\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  padding 1\n",
      "   new size padding 1: (13312, 13312)\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (13312, 13312)\n",
      "len list: 676\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (13824, 13312)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (13312, 13824)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (13824, 13824)\n",
      "len list: 729\n",
      "  save samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'arr_area' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-af5245f8d0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0marr_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr_area' is not defined"
     ]
    }
   ],
   "source": [
    "arr_label_dev = zero_padding_1_labels(arr_label_dev, size_sub_sample)\n",
    "\n",
    "\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    arr_area_2 = zero_padding_2_labels(arr_label_dev, dim_str, inv_stride, size_sub_sample)\n",
    "    list_arrs = split_array_2D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    print('len list:', len(list_arrs))\n",
    "    save_samples(list_arrs, dir_label_dev, 'split', area_file_label_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "del arr_area\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Mosul_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "takes 20 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_val_hfd5, file_val_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "#name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_val_fd5, file_val_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "#name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_test_hfd5, file_test_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# no labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3037, 3037)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__(), len(my_training_batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6075, 512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_hdf5_X, 'r') as f1_Y:\n",
    "    dataset_Y  = f1_Y['all_train_data_X']\n",
    "    print(dataset_Y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H = model.fit(dataset_X, dataset_Y,  \n",
    "                 batch_size=BS, \n",
    "                epochs=EPOCHS, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### load data dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_hdf5_Y, 'r') as f1_Y:\n",
    "    dsety  = f1_Y['all_train_data_Y']\n",
    "    dataset_Y = dsety[:100]\n",
    "    \n",
    "dataset_Y = dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1)#.copy()\n",
    "\n",
    "with h5py.File(file_hdf5_X, 'r') as f1_X:\n",
    "    dsetx = f1_X['all_train_data_X']\n",
    "    dataset_X = dsetx[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 512, 512, 1), (100, 512, 512, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_X_1 = dataset_X[:,:,:,0:1]#.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 81156)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset_X_1), np.sum(dataset_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 512, 512, 1), (32, 512, 512, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.067109008, 0.067109008)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(dataset_X_1) / 10**9, sys.getsizeof(dataset_Y) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  ' channels).')\n",
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "train_datagen_aug = get_augmented(\n",
    "        dataset_X,\n",
    "        dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1),\n",
    "        X_val=None,\n",
    "        Y_val=None,\n",
    "        batch_size=BS,\n",
    "        seed=0,\n",
    "        data_gen_args=dict(\n",
    "            rotation_range=90,\n",
    "            height_shift_range=0,\n",
    "            shear_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir_train_X_split = join(dir_temp_train_X, 'split')\n",
    "dir_train_Y_split = join(dir_temp_train_Y, 'split')\n",
    "\n",
    "dir_val_X_split = join(dir_temp_val_X, 'split')\n",
    "dir_val_Y_split = join(dir_temp_val_Y, 'split')\n",
    "\n",
    "dir_test_X_split = join(dir_temp_test_X, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, dir_X, dir_Y, batch_size):\n",
    "        self.list_files_X = [join(dir_X, file_name) for file_name in os.listdir(dir_X)]\n",
    "        self.list_files_Y = [join(dir_Y, file_name) for file_name in os.listdir(dir_Y)]\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.list_files_X) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_in = self.list_files_X[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y_in = self.list_files_Y[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        b_x = np.array([np.load(file_name) for file_name in batch_x_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        b_y = np.array([np.load(file_name) for file_name in batch_y_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        \n",
    "        return (b_x.reshape(b_x.shape[0], b_x.shape[2], b_x.shape[3], b_x.shape[1]), \n",
    "                b_y.reshape(b_y.shape[0], b_y.shape[1], b_y.shape[2], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(dir_train_X_split, dir_train_Y_split, BS)\n",
    "my_validation_batch_gen = My_Custom_Generator(dir_val_X_split, dir_val_Y_split, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__() * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 512, 512, 8) (31, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "filepath =join(path_model, \"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\")\n",
    "\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "#cb_tb = keras_cb.tensorboard_v1.TensorBoard(log_dir=join(path_model, 'logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            #write_graph=True, write_grads=True, write_images=True, update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(my_training_batch_gen.list_files_X) // BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=len(my_training_batch_gen.list_files_X) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2,\n",
    "#                         validation_data = my_validation_batch_gen,\n",
    "#                         validation_steps = len(my_validation_batch_gen.list_files_X) // BS\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(join(path_model, \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_datagen = get_augmented(\n",
    "#         trainX,\n",
    "#         trainY,\n",
    "#         X_val=None,\n",
    "#         Y_val=None,\n",
    "#         batch_size=BS,\n",
    "#         seed=0,\n",
    "#         data_gen_args=dict(\n",
    "#             rotation_range=90,\n",
    "#             height_shift_range=0,\n",
    "#             shear_range=0,\n",
    "#             horizontal_flip=True,\n",
    "#             vertical_flip=True,\n",
    "#             fill_mode='constant'\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "\n",
    "BS = 2\n",
    "EPOCHS = 2\n",
    "size_sub_sample = (512, 512)\n",
    "\n",
    "inv_stride = 2\n",
    "INIT_LR = 1e-3\n",
    "depth_img = 8\n",
    "\n",
    "IMAGE_DIMS = size_sub_sample + (depth_img,)\n",
    "model_init = satellite_unet(IMAGE_DIMS, num_layers=4)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "print(IMAGE_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "3037 3037\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>1:\n",
    "        break\n",
    "        \n",
    "print(my_training_batch_gen.__len__(), len(my_training_batch_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath_chk, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "cb_tb = keras_cb.TensorBoard(log_dir=join(path_model, 'tensoboard_logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            write_graph=True, write_grads=True, write_images=True, \n",
    "                                            update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]# cb_tb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_epoch(filepath_chk):\n",
    "    return int(filepath_chk.split('-')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_epoch: 0\n"
     ]
    }
   ],
   "source": [
    "#checkpoints\n",
    "filepath_chk = None\n",
    "for i, file in enumerate(os.listdir(path_model)): \n",
    "    if i == len(os.listdir(path_model)) - 1:\n",
    "        if file != '.DS_Store':\n",
    "            filepath_chk = join(path_model, file)\n",
    "            print('checkpoint use: {}'.format(filepath_chk))\n",
    "            \n",
    "            \n",
    "if filepath_chk is not None:\n",
    "    \n",
    "    # Load model:\n",
    "    print('loading model')\n",
    "    model = load_model(filepath_chk)\n",
    "    \n",
    "    # Finding the epoch index from which we are resuming\n",
    "    print('find epoch')\n",
    "    initial_epoch = get_init_epoch(filepath_chk)\n",
    "    \n",
    "    # Calculating the correct value of count\n",
    "    count = initial_epoch*batches_per_epoch_test\n",
    "    # Update the value of count in callback instance\n",
    "    callbacks_list[1].count = count\n",
    "\n",
    " \n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-2)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    \n",
    "    \n",
    "else:\n",
    "#   model = model_init\n",
    "    filepath_chk = join(path_model, \"weights-improvement-{epoch:02d}-{iou:.3f}.hdf5\")\n",
    "    initial_epoch = 0\n",
    "    model = model_init\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])\n",
    "    \n",
    "print('initial_epoch:', initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 3/10 [========>.....................] - ETA: 6:57 - loss: 0.8438 - iou: 0.0237 - iou_thresholded: 0.0272 "
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=batches_per_epoch_test,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=1,\n",
    "                        #use_multiprocessing=True,\n",
    "                        #workers=6, \n",
    "                        #max_queue_size=32,\n",
    "                        initial_epoch=initial_epoch\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file) as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "file_plot_train_info = join(path_model, 'info_train_{}.png'.format(utl.datetime_now()))\n",
    "plt.savefig(file_plot_train_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_unosatenv]",
   "language": "python",
   "name": "conda-env-conda_unosatenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
