{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save np.arrays as h5 files instead of .npy\n",
    "# TODO: generator input deep learning\n",
    "# TODO: post processing: reconstruct image from network output and create shape files\n",
    "# TODO: contruct my own f1 score definition, perhaps take iou=0.5 per polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install pandas\n",
    "    !pip install matplotlib\n",
    "    !pip install rasterio\n",
    "    !pip install shapely\n",
    "    !pip install descartes\n",
    "    !pip install pyproj\n",
    "    !pip install geopandas\n",
    "    !pip install keras_unet\n",
    "    !pip install h5py\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    js = ('<script>function ConnectButton(){ '\n",
    "            'console.log(\"Connect pushed\"); '\n",
    "            'document.querySelector(\"#connect\").click()} '\n",
    "            'setInterval(ConnectButton,3000);</script>')\n",
    "    display(HTML(js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from rasterio.mask import mask\n",
    "from os.path import join\n",
    "import descartes\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import fiona \n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras import callbacks as keras_cb \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# see: https://github.com/karolzak/keras-unet\n",
    "from keras_unet.models import satellite_unet \n",
    "from keras_unet.losses import jaccard_distance\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.utils import get_augmented\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(sys.executable)\n",
    "\n",
    "if colab:\n",
    "    sys.path.append(\"/content/drive/My Drive/app_UNOSAT\")\n",
    "    #sys.path.append(\"/content/drive/My Drive/app_UNOSAT/src\")\n",
    "\n",
    "from src import load_data as ld\n",
    "from src import preprocessing as pp\n",
    "from src import prepare_for_network as pfn\n",
    "from src import utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(pfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths, set your own path here\n",
    "if colab:\n",
    "    path_main = '/content/drive/My Drive/app_UNOSAT'\n",
    "else:\n",
    "    path_main = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge'\n",
    "\n",
    "path_data_main = utl.make_dir(join(path_main, \"data\"))\n",
    "\n",
    "#### put data in these dirs\n",
    "\n",
    "# I choose:\n",
    "# Training: Mosul, Najaf, Nasiryah\n",
    "# Validation: Souleimaniye\n",
    "# Test: Bagdad, Kirkouk, Samawah, Tikrit (no labels present)\n",
    "path_data_local_train = join(path_data_main, 'Train_Dataset')\n",
    "path_data_local_val = join(path_data_main, 'Validation_Dataset')\n",
    "path_data_local_test = join(path_data_main, 'Evaluation_Dataset')\n",
    "\n",
    "path_model =  utl.make_dir(join(path_data_main, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dirs are made automatically\n",
    "dir_temp_data =  utl.make_dir(join(path_data_main, 'data_temp'))\n",
    "dir_temp_train = utl.make_dir(join(dir_temp_data, 'train'))\n",
    "dir_temp_val = utl.make_dir(join(dir_temp_data, 'val'))\n",
    "dir_temp_test = utl.make_dir(join(dir_temp_data, 'test'))\n",
    "dir_temp_plots = utl.make_dir(join(dir_temp_data, 'plots_preprocessing'))\n",
    "\n",
    "dir_temp_train_X = utl.make_dir(join(dir_temp_train, 'data'))\n",
    "dir_temp_train_Y = utl.make_dir(join(dir_temp_train, 'labels'))\n",
    "\n",
    "dir_temp_val_X = utl.make_dir(join(dir_temp_val, 'data'))\n",
    "dir_temp_val_Y = utl.make_dir(join(dir_temp_val, 'labels'))\n",
    "\n",
    "dir_temp_test_X = utl.make_dir(join(dir_temp_data, 'data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files in 'Train_Dataset' are automatically selected for training (saved in sub dir 'whole'). All files in 'Validation_Dataset' are automatically selected for validation. All files in 'Evaluation_Dataset' are automatically selected for testing.<br>\n",
    "Data (.tif files) and labels (.shp files) are both converted to .npy/.h5 files. First preprocessing is done for the .tif files (clip at quantile 'quantile_clip_max'). The label files become mask arrays, with 1=urban area and 0=non-urban area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings preprocessing\n",
    "quantile_clip_max = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros_xy_single_side(arr, size_sub_sample):\n",
    "    shape_x, shape_y = arr.shape\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    print(\"arr.shape:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "    \n",
    "    #print('new shape:', arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'i8'\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "path_file_hfd5 = join(dir_temp_train,  'groups_test_X.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Najaf\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Nasiryah\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with hdf5\n",
    "dtype = 'i8'\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "path_file_hfd5 = join(dir_temp_train,  'groups_test_X.hdf5')\n",
    "\n",
    "with h5py.File(path_file_hfd5, 'w') as f:\n",
    "    for area in dict_paths:\n",
    "        print(area)\n",
    "        i = 0\n",
    "        #g = f.create_group(area)\n",
    "        dir_save_plots = utl.make_dir(join(dir_temp_plots, area))        \n",
    "        \n",
    "        for season in ld.seasons_fixed_order:\n",
    "            for pol in sorted(dict_paths[area]['tif'][season]):\n",
    "                print(' ', season, pol)\n",
    "                path_raster = dict_paths[area]['tif'][season][pol]\n",
    "                np_arr_processed = pp.process_image(rio.open(path_raster).read()[0], \n",
    "                                                    quantile_clip_max,\n",
    "                                                    clip_min=0, \n",
    "                                                    plotting=True, \n",
    "                                                    dir_save_plots=dir_save_plots,\n",
    "                                                    area=area,  season=season, pol=pol)\n",
    "\n",
    "                shape_x, shape_y = np_arr_processed.shape\n",
    "                dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "                dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "                np_arr_processed = pad_zeros_xy(np_arr_processed, dim_x_add, dim_y_add)\n",
    "                \n",
    "                #np_arr_shape_new = np_arr_processed.shape + (1,)\n",
    "                stacked_shape = np_arr_processed.shape  + (8,)        \n",
    "                \n",
    "                if i == 0:\n",
    "                    dset = f.create_dataset(area, \n",
    "                                            stacked_shape,\n",
    "                                            dtype=dtype,\n",
    "                                            #maxshape=stacked_shape,\n",
    "                                            chunks=True)\n",
    "                    print(' shape init', dset.shape)\n",
    "                    \n",
    "                dset[:, :, i] = np_arr_processed#.reshape(np_arr_shape_new)\n",
    "                i += 1\n",
    "                \n",
    "        print(\"shape dataset:\", dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_file_hfd5, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Mosul\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Najaf\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Nasiryah\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 4316046\n",
      "polygon mask shape: (10980, 10980)\n",
      "sum polygon_mask_int: 3709703\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 1228702\n",
      "stack and save arrays...\n",
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n",
      "save labels...\n",
      "takes 5 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_train = ld.paths_in_dict(path_data_local_train)\n",
    "\n",
    "dict_raster_layers_train, dict_data_train = pp.preprocess_data(dict_paths_train, dir_temp_plots, quantile_clip_max)\n",
    "dict_labels_train = pp.get_label_data(dict_paths_train, dict_raster_layers_train)\n",
    "list_paths_train_X = pp.save_stacked_arrays(dict_data_train, dir_temp_train_X)\n",
    "list_paths_train_Y = pp.save_labels(dict_labels_train, dir_temp_train_Y)\n",
    "\n",
    "del dict_raster_layers_train\n",
    "del dict_data_train\n",
    "del dict_labels_train\n",
    "gc.collect()\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Souleimaniye\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10979, 10980)\n",
      "sum polygon_mask_int: 1861770\n",
      "stack and save arrays...\n",
      "Souleimaniye\n",
      "save labels...\n",
      "takes 2 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_val = ld.paths_in_dict(path_data_local_val)\n",
    "\n",
    "dict_raster_layers_val, dict_data_val = pp.preprocess_data(dict_paths_val, dir_temp_plots, quantile_clip_max)\n",
    "dict_labels_val = pp.get_label_data(dict_paths_val, dict_raster_layers_val)\n",
    "list_paths_val_X = pp.save_stacked_arrays(dict_data_val, dir_temp_val_X)\n",
    "list_paths_val_Y = pp.save_labels(dict_labels_val, dir_temp_val_Y)\n",
    "\n",
    "del dict_raster_layers_val\n",
    "del dict_data_val\n",
    "del dict_labels_val\n",
    "gc.collect()\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "dict_paths_test = ld.paths_in_dict(path_data_local_test)\n",
    "\n",
    "dict_raster_layers_test, dict_data_test = pp.preprocess_data(dict_paths_test, dir_temp_plots, quantile_clip_max)\n",
    "list_paths_test_X = pp.save_stacked_arrays(dict_data_test, dir_temp_test_X)\n",
    "\n",
    "del dict_raster_layers_test\n",
    "del dict_data_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and labels are in .npy / .h5 file shape now, but they need to be converted to suitable input for a network.\n",
    "Therefore a full image is cut into many sub-samples of size 'size_sub_sample', e.g. (512, 512). This requires first zero padding (func zero_padding_1) in order to fit the image to an integer times size_sub_sample. <br>\n",
    "Then we augment data by using a stride of size_sub_sample/inv_stride, i.e. we shift the image by for example half a sample size (inv_stride=2). Subsequently we cut into sub-images.  This results in inv_stride<sup>2</sup> times more data. In order to fit n sub-samples into a shifted image, new padding is required (func zero_padding_2). <br>\n",
    "Output is saved in sub dir 'split'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size_sub_sample = (256, 256)\n",
    "size_sub_sample = (512, 512)\n",
    "inv_stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_hd5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0e4d9badf97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_hd5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_hd5' is not defined"
     ]
    }
   ],
   "source": [
    "def get_all(name):\n",
    "    print(name)\n",
    "\n",
    "def get_shape(ds):\n",
    "    print(ds.shape)\n",
    "    \n",
    "with h5py.File(path_hd5, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        ds = f[key]\n",
    "        print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    #print('padding 2:')\n",
    "    #print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    #print(\" new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices of shape (nrows, ncols)\"\"\"\n",
    "    r, h = array.shape\n",
    "    return (array.reshape(h//nrows, nrows, -1, ncols)\n",
    "                 .swapaxes(1, 2)\n",
    "                 .reshape(-1, nrows, ncols))\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "(11264, 11264, 8)\n",
      "_\n",
      "n_samples: 484\n",
      "dset2.shape: (484, 512, 512, 8)\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#path_hd5 = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/data_temp/train/groups_test_X.hdf5'\n",
    "path_file2_hd5 = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/data_temp/train/groups2_test_X.hdf5'\n",
    "\n",
    "with h5py.File(path_file2_hd5, 'w') as f2:\n",
    "    with h5py.File(path_file_hfd5, 'r') as f1:\n",
    "\n",
    "        for area in f1.keys():\n",
    "            print(area)\n",
    "            dset = f1[area]\n",
    "            print(dset.shape)\n",
    "\n",
    "            g = f2.create_group('area')\n",
    "\n",
    "            for dim_str in ['_', 'x', 'y', 'xy']:\n",
    "                print(dim_str)\n",
    "                np_arr_shape = list(dset.shape)\n",
    "                if 'x' in dim_str:\n",
    "                    np_arr_shape[0] = np_arr_shape[0] + size_sub_sample[0]\n",
    "                if 'y' in dim_str:\n",
    "                    np_arr_shape[1] = np_arr_shape[1] + size_sub_sample[1]\n",
    "                \n",
    "                n_samples = int(np_arr_shape[0] * np_arr_shape[1] / (size_sub_sample[0] * size_sub_sample[1]))\n",
    "                print(\"n_samples:\", n_samples)\n",
    "                np_arr_shape_split = (n_samples,) + size_sub_sample + (8,)\n",
    "                \n",
    "                dset2 = g.create_dataset(dim_str, \n",
    "                                            np_arr_shape_split,\n",
    "                                            dtype=dtype,\n",
    "                                            chunks=True)\n",
    "                \n",
    "                print(\"dset2.shape:\", dset2.shape)\n",
    "                for i in range(8):\n",
    "                    print(i)\n",
    "                    #arr_new = zero_padding_2(dset[:,:,i], dim_str, inv_stride, size_sub_sample) \n",
    "                    #print(\"arr_new.shape:\", arr_new.shape)\n",
    "                    dset2[:,:,:,i] = split_array_2D(zero_padding_2(dset[:,:,i], \n",
    "                                                                   dim_str,\n",
    "                                                                   inv_stride, \n",
    "                                                                   size_sub_sample), \n",
    "                                                    size_sub_sample[0], \n",
    "                                                    size_sub_sample[1])\n",
    "                print(\"shape dataset:\", dset.shape)\n",
    "\n",
    "print(\"total time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 10979, 8, 284, 10979, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File(path_file2_hd5, 'w') as f2:\n",
    "    with h5py.File(path_file_hfd5, 'r') as f1:\n",
    "\n",
    "        for area in f1.keys():\n",
    "            print(area)\n",
    "            dset = f1[area]\n",
    "            print(dset.shape)\n",
    "\n",
    "            g = f2.create_group('area')\n",
    "\n",
    "            for dim_str in ['_', 'x', 'y', 'xy']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "def get_all(name):\n",
    "    print(name)\n",
    "         \n",
    "with h5py.File(path_hd5, 'r') as f:\n",
    "    f.visit(get_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_label_dev = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/data_temp/train/labels'\n",
    "area_file_label_dev = 'Mosul_label.npy'\n",
    "arr_label_dev = np.load(join(dir_label_dev, 'whole', area_file_label_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-696fb436eb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print(arr.shape)\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fc3d05e335f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdim_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_y_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_y\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_y_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new size padding 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cc25739f4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minv_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print('x1:', arr.shape)\n",
    "dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "print('x2:', arr.shape)\n",
    "\n",
    "dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "sys.getsizeof(arr) / 10**9, arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding_1(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print(arr.shape)\n",
    "    dim_x = arr.shape[1]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "    dim_y = arr.shape[2]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "    print(\"new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "    \n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('padding 2:')\n",
    "    print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            print('x1:', arr.shape)\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "            print('x2:', arr.shape)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "    print(\" new size array:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])\n",
    "\n",
    "def save_array_in_new_sub_dir(arr, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('save_array_in_new_sub_dir')\n",
    "    name_new_dir = 'split'\n",
    "    dir_data_split = join(main_dir, name_new_dir) \n",
    "    ld.make_dir(dir_data_split)\n",
    "    path_file_split = join(dir_data_split, os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str)\n",
    "    np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices and .\"\"\"\n",
    "    r, h = array.shape\n",
    "    arr_3D = array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
    "    return [arr for arr in arr_3D]\n",
    "\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                             .swapaxes(1, 2)\n",
    "                             .reshape(-1, nrows, ncols))\n",
    "    \n",
    "        arr_4D = np.array(list_3D_arrs)\n",
    "        \n",
    "    arr_4D = arr_4D.reshape(arr_4D.shape[1], arr_4D.shape[0], arr_4D.shape[2], arr_4D.shape[3])\n",
    "    return [arr for arr in arr_4D]\n",
    "\n",
    "def save_samples(list_arrs, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('  save samples')\n",
    "    dir_data_split = join(main_dir, name_new_dir)\n",
    "    ld.make_dir(dir_data_split)\n",
    "\n",
    "    str_filename =  os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str + '_{}' \n",
    "    for i, arr in enumerate(list_arrs):\n",
    "        path_file_split = join(dir_data_split, str_filename.format(i))\n",
    "        np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "def zero_padding_1_labels(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print('  padding 1')\n",
    "    dim_x = arr.shape[0]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "\n",
    "    dim_y = arr.shape[1]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "    print(\"   new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def zero_padding_2_labels(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('  padding 2:')\n",
    "    print('    dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    print(\"   new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new size padding 1: (8, 11264, 11264)\n"
     ]
    }
   ],
   "source": [
    "# zero padding 1\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding 2:\n",
      " dim_str: \n",
      " new size array: (8, 11264, 11264)\n",
      "pad 2 4.0600864 (8, 11264, 11264)\n",
      "len list: 484\n",
      "shape arr: (8, 512, 512)\n",
      "  save samples\n",
      "padding 2:\n",
      " dim_str: x\n",
      "x1: (8, 11264, 11264)\n",
      "x2: (8, 11776, 11264)\n",
      " new size array: (8, 11776, 11264)\n",
      "pad 2 4.244635776 (8, 11776, 11264)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e17f249c2db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlist_arrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_array_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_area_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'len list:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape arr:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a2e7486f08b8>\u001b[0m in \u001b[0;36msplit_array_3D\u001b[0;34m(array, nrows, ncols)\u001b[0m\n\u001b[1;32m     16\u001b[0m         list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n\u001b[1;32m     17\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                              .reshape(-1, nrows, ncols))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0marr_4D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_3D_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rest of pipeline\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    #zero padding 2\n",
    "    arr_area_2 = zero_padding_2(arr, dim_str, inv_stride, size_sub_sample)\n",
    "    print('pad 2',sys.getsizeof(arr_area_2) / 10**9, arr_area_2.shape)\n",
    "\n",
    "    #split\n",
    "    list_arrs = split_array_3D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    print('len list:', len(list_arrs))\n",
    "    print('shape arr:', list_arrs[0].shape)\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    \n",
    "    save_samples(list_arrs, dir_data_dev, 'split', area_file_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "    \n",
    "del arr\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  padding 1\n",
      "   new size padding 1: (13312, 13312)\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (13312, 13312)\n",
      "len list: 676\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (13824, 13312)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (13312, 13824)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (13824, 13824)\n",
      "len list: 729\n",
      "  save samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'arr_area' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-af5245f8d0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0marr_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr_area' is not defined"
     ]
    }
   ],
   "source": [
    "arr_label_dev = zero_padding_1_labels(arr_label_dev, size_sub_sample)\n",
    "\n",
    "\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    arr_area_2 = zero_padding_2_labels(arr_label_dev, dim_str, inv_stride, size_sub_sample)\n",
    "    list_arrs = split_array_2D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    print('len list:', len(list_arrs))\n",
    "    save_samples(list_arrs, dir_label_dev, 'split', area_file_label_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "del arr_area\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Mosul_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "takes 20 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# data\n",
    "for area_data_file in os.listdir(join(dir_temp_train_X, 'whole')):\n",
    "    pfn.pipeline_prepare_for_nn(area_data_file, dir_temp_train_X, inv_stride, size_sub_sample)\n",
    "    gc.collect()\n",
    "# labels\n",
    "for area_label_file in os.listdir(join(dir_temp_train_Y, 'whole')):\n",
    "    pfn.pipeline_prepare_for_nn_labels(area_label_file, dir_temp_train_Y, inv_stride, size_sub_sample)\n",
    "    gc.collect()\n",
    "    \n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Souleimaniye.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Souleimaniye_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "takes 6 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# data\n",
    "for area_data_file in os.listdir(join(dir_temp_val_X, 'whole')):\n",
    "    pfn.pipeline_prepare_for_nn(area_data_file, dir_temp_val_X, inv_stride, size_sub_sample)\n",
    "    gc.collect()\n",
    "# labels\n",
    "for area_label_file in os.listdir(join(dir_temp_val_Y, 'whole')):\n",
    "    pfn.pipeline_prepare_for_nn_labels(area_label_file, dir_temp_val_Y, inv_stride, size_sub_sample)\n",
    "    gc.collect()\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "for area_data_file in os.listdir(join(dir_temp_data_test_X, 'whole')):\n",
    "    pfn.pipeline_prepare_for_nn(area_data_file, dir_temp_data_test_X, inv_stride, size_sub_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "inv_stride = 2\n",
    "EPOCHS = 1\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "IMAGE_DIMS =  size_sub_sample + (8,)\n",
    "model = satellite_unet(IMAGE_DIMS)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train_X_split = join(dir_temp_train_X, 'split')\n",
    "dir_train_Y_split = join(dir_temp_train_Y, 'split')\n",
    "\n",
    "dir_val_X_split = join(dir_temp_val_X, 'split')\n",
    "dir_val_Y_split = join(dir_temp_val_Y, 'split')\n",
    "\n",
    "dir_test_X_split = join(dir_temp_test_X, 'split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, dir_X, dir_Y, batch_size):\n",
    "        self.list_files_X = [join(dir_X, file_name) for file_name in os.listdir(dir_X)]\n",
    "        self.list_files_Y = [join(dir_Y, file_name) for file_name in os.listdir(dir_Y)]\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.list_files_X) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_in = self.list_files_X[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y_in = self.list_files_Y[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        b_x = np.array([np.load(file_name) for file_name in batch_x_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        b_y = np.array([np.load(file_name) for file_name in batch_y_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        \n",
    "        return (b_x.reshape(b_x.shape[0], b_x.shape[2], b_x.shape[3], b_x.shape[1]), \n",
    "                b_y.reshape(b_y.shape[0], b_y.shape[1], b_y.shape[2], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(dir_train_X_split, dir_train_Y_split, BS)\n",
    "my_validation_batch_gen = My_Custom_Generator(dir_val_X_split, dir_val_Y_split, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 512, 512, 8) (31, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "filepath =join(path_model, \"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\")\n",
    "\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "#cb_tb = keras_cb.tensorboard_v1.TensorBoard(log_dir=join(path_model, 'logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            #write_graph=True, write_grads=True, write_images=True, update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_training_batch_gen.list_files_X) // BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=len(my_training_batch_gen.list_files_X) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2,\n",
    "#                         validation_data = my_validation_batch_gen,\n",
    "#                         validation_steps = len(my_validation_batch_gen.list_files_X) // BS\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(join(path_model, \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'plots_preprocessing', 'test', 'train', 'val']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dir_temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('load data...')\n",
    "if test_run:\n",
    "    # testing with one area\n",
    "    trainX = np.load(file_X)\n",
    "    trainY = np.load(file_Y)\n",
    "else:\n",
    "    # for real with all areas, big data\n",
    "    trainX = ld.load_entire_dataset(join(dir_temp_data_train, 'split'))\n",
    "    trainY = ld.load_entire_dataset(join(dir_temp_labels, 'split'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[2], trainX.shape[3], trainX.shape[1])\n",
    "trainY = trainY.reshape(trainY.shape[0], trainY.shape[1], trainY.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = get_augmented(\n",
    "        trainX,\n",
    "        trainY,\n",
    "        X_val=None,\n",
    "        Y_val=None,\n",
    "        batch_size=BS,\n",
    "        seed=0,\n",
    "        data_gen_args=dict(\n",
    "            rotation_range=90,\n",
    "            height_shift_range=0,\n",
    "            shear_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =\"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit_generator(generator=train_datagen,\n",
    "                        steps_per_epoch=len(trainX) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1015021568 into shape (8,512,484)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5a9170367f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1015021568 into shape (8,512,484)"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_unosatenv]",
   "language": "python",
   "name": "conda-env-conda_unosatenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
