{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save np.arrays as h5 files instead of .npy\n",
    "# TODO: generator input deep learning\n",
    "# TODO: post processing: reconstruct image from network output and create shape files\n",
    "# TODO: contruct my own f1 score definition, perhaps take iou=0.5 per polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install pandas\n",
    "    !pip install matplotlib\n",
    "    !pip install rasterio\n",
    "    !pip install shapely\n",
    "    !pip install descartes\n",
    "    !pip install pyproj\n",
    "    !pip install geopandas\n",
    "    !pip install keras_unet\n",
    "    !pip install h5py\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    js = ('<script>function ConnectButton(){ '\n",
    "            'console.log(\"Connect pushed\"); '\n",
    "            'document.querySelector(\"#connect\").click()} '\n",
    "            'setInterval(ConnectButton,3000);</script>')\n",
    "    display(HTML(js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from rasterio.mask import mask\n",
    "from os.path import join\n",
    "import descartes\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import fiona \n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras import callbacks as keras_cb \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# see: https://github.com/karolzak/keras-unet\n",
    "from keras_unet.models import satellite_unet \n",
    "from keras_unet.losses import jaccard_distance\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.utils import get_augmented\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# # prevent Tensorflow memory leakage\n",
    "# K.clear_session()\n",
    "\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "print(sys.executable)\n",
    "\n",
    "if colab:\n",
    "    sys.path.append(\"/content/drive/My Drive/app_UNOSAT\")\n",
    "    #sys.path.append(\"/content/drive/My Drive/app_UNOSAT/src\")\n",
    "\n",
    "from src import load_data as ld\n",
    "from src import preprocessing as pp\n",
    "from src import prepare_for_network as pfn\n",
    "from src import utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(pfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths, set your own path here\n",
    "if colab:\n",
    "    #path_main = '/content/drive/My Drive/app_UNOSAT'\n",
    "    path_main = '/content/drive/Computers/My Macbook Pro'\n",
    "else:\n",
    "    path_main = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge'\n",
    "\n",
    "path_data_main = utl.make_dir(join(path_main, \"data\"))\n",
    "\n",
    "#### put data in these dirs\n",
    "\n",
    "# I choose:\n",
    "# Training: Mosul, Najaf, Nasiryah\n",
    "# Validation: Souleimaniye\n",
    "# Test: Bagdad, Kirkouk, Samawah, Tikrit (no labels present)\n",
    "path_data_local_train = join(path_data_main, 'Train_Dataset')\n",
    "path_data_local_val = join(path_data_main, 'Validation_Dataset')\n",
    "path_data_local_test = join(path_data_main, 'Evaluation_Dataset')\n",
    "\n",
    "path_model =  utl.make_dir(join(path_data_main, 'model'))\n",
    "path_checkpoints =  utl.make_dir(join(path_model, 'checkpoints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dirs are made automatically\n",
    "dir_temp_data =  utl.make_dir(join(path_data_main, 'data_temp'))\n",
    "dir_temp_train = utl.make_dir(join(dir_temp_data, 'train'))\n",
    "dir_temp_val = utl.make_dir(join(dir_temp_data, 'val'))\n",
    "dir_temp_dev = utl.make_dir(join(dir_temp_data, 'data_dev'))\n",
    "dir_temp_eval = utl.make_dir(join(dir_temp_data, 'evaluation'))\n",
    "dir_temp_plots = utl.make_dir(join(dir_temp_data, 'plots_preprocessing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings preprocessing\n",
    "quantile_clip_max = 0.999\n",
    "size_sub_sample = (512, 512)\n",
    "dtype = 'i16'\n",
    "inv_stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All files in 'Train_Dataset' are automatically selected for training (saved in sub dir 'whole'). All files in 'Validation_Dataset' are automatically selected for validation. All files in 'Evaluation_Dataset' are automatically selected for testing.<br>\n",
    "Data (.tif files) and labels (.shp files) are both converted to .hdf5 files. First preprocessing is done for the .tif files (clip at quantile 'quantile_clip_max'). The label files become mask arrays, with 1=urban area and 0=non-urban area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pad_zeros_xy_single_side(arr, size_sub_sample):\n",
    "    shape_x, shape_y = arr.shape\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    print(\"arr.shape:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "    \n",
    "    #print('new shape:', arr.shape)\n",
    "    return arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dtype = 'i8'\n",
    "#dtype = 'i16'\n",
    "sub_sample_shape = (512, 512)\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "file_hfd5 = join(dir_temp_dev,  'testrun_X.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Najaf\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n",
      "Nasiryah\n",
      "  winter vh\n",
      "new shape: (11264, 11264)\n",
      " shape init (11264, 11264, 8)\n",
      "  winter vv\n",
      "new shape: (11264, 11264)\n",
      "  spring vh\n",
      "new shape: (11264, 11264)\n",
      "  spring vv\n",
      "new shape: (11264, 11264)\n",
      "  summer vh\n",
      "new shape: (11264, 11264)\n",
      "  summer vv\n",
      "new shape: (11264, 11264)\n",
      "  autumn vh\n",
      "new shape: (11264, 11264)\n",
      "  autumn vv\n",
      "new shape: (11264, 11264)\n",
      "shape dataset: (11264, 11264, 8)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with hdf5\n",
    "dtype = 'i8'\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n",
    "path_file_hfd5 = join(dir_temp_train,  'groups_test_X.hdf5')\n",
    "\n",
    "with h5py.File(file_hfd5, 'w') as f:\n",
    "    for area in dict_paths:\n",
    "        print(area)\n",
    "        i = 0\n",
    "        #g = f.create_group(area)\n",
    "        dir_save_plots = utl.make_dir(join(dir_temp_plots, area))        \n",
    "        \n",
    "        for season in ld.seasons_fixed_order:\n",
    "            for pol in sorted(dict_paths[area]['tif'][season]):\n",
    "                print(' ', season, pol)\n",
    "                path_raster = dict_paths[area]['tif'][season][pol]\n",
    "                np_arr_processed = pp.process_image(rio.open(path_raster).read()[0], \n",
    "                                                    quantile_clip_max,\n",
    "                                                    clip_min=0, \n",
    "                                                    plotting=True, \n",
    "                                                    dir_save_plots=dir_save_plots,\n",
    "                                                    area=area,  season=season, pol=pol)\n",
    "\n",
    "                shape_x, shape_y = np_arr_processed.shape\n",
    "                dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "                dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "                \n",
    "                #np_arr_processed = pad_zeros_xy(np_arr_processed, dim_x_add, dim_y_add)\n",
    "                #np_arr_shape_new = np_arr_processed.shape + (1,)\n",
    "                stacked_shape = (shape_x + dim_x_add, shape_y + dim_y_add)  + (8,)        \n",
    "                \n",
    "                if i == 0:\n",
    "                    dset = f.create_dataset(area, \n",
    "                                            stacked_shape,\n",
    "                                            dtype=dtype,\n",
    "                                            chunks=True)\n",
    "                    print(' shape init', dset.shape)\n",
    "                    \n",
    "                dset[:, :, i] = pad_zeros_xy(np_arr_processed, dim_x_add, dim_y_add)\n",
    "                i += 1\n",
    "                \n",
    "        print(\"shape dataset:\", dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_file_hfd5, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hfd5_labels = join(dir_temp_dev, 'testrun_Y.hdf5')\n",
    "dict_paths = ld.paths_in_dict(path_data_local_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 4316046\n",
      "Najaf\n",
      "polygon mask shape: (10980, 10980)\n",
      "sum polygon_mask_int: 3709703\n",
      "Nasiryah\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 1228702\n"
     ]
    }
   ],
   "source": [
    "def shp2polygons(dict_paths):\n",
    "    dict_polygons = dict()   \n",
    "    for area in dict_paths:\n",
    "        path_shape = dict_paths[area]['shp']\n",
    "        dict_polygons[area] = gpd.read_file(path_shape)\n",
    "    return dict_polygons\n",
    "                \n",
    "def preprocess_shape(dict_paths, file_hfd5_labels, sub_sample_shape):\n",
    "    with h5py.File(file_hfd5_labels, 'w') as f:\n",
    "        for area in dict_paths:\n",
    "            print(area)\n",
    "\n",
    "            dict_polygons = shp2polygons(dict_paths)\n",
    "\n",
    "            # raster obj only required for shape and crs, this is season or pol independent\n",
    "            path_raster = dict_paths[area]['tif']['spring']['vh']\n",
    "            raster_obj = rio.open(path_raster)\n",
    "            arr_shape = raster_obj.shape\n",
    "\n",
    "            dim_x_add = sub_sample_shape[0] - arr_shape[0] % sub_sample_shape[0]\n",
    "            dim_y_add = sub_sample_shape[1] - arr_shape[1] % sub_sample_shape[1]       \n",
    "            arr_new_shape = (arr_shape[0] + dim_x_add,  arr_shape[1] + dim_y_add)\n",
    "\n",
    "            dset = f.create_dataset(area, \n",
    "                                    arr_new_shape,\n",
    "                                    dtype=dtype,\n",
    "                                    chunks=True)\n",
    "\n",
    "            dset[:] = pad_zeros_xy(pp.polygons_2_np_arr_mask(dict_polygons[area], \n",
    "                                                                 raster_obj),\n",
    "                                   dim_x_add,\n",
    "                                   dim_y_add\n",
    "                                  )       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Mosul\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Najaf\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "Nasiryah\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 4316046\n",
      "polygon mask shape: (10980, 10980)\n",
      "sum polygon_mask_int: 3709703\n",
      "polygon mask shape: (10980, 10979)\n",
      "sum polygon_mask_int: 1228702\n",
      "stack and save arrays...\n",
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n",
      "save labels...\n",
      "takes 5 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_train = ld.paths_in_dict(path_data_local_train)\n",
    "\n",
    "file_hfd5_train_X = join(dir_temp_train, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_train_Y = join(dir_temp_train, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_train, file_hfd5_train_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_train, file_hfd5_train_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Souleimaniye\n",
      "  winter vh\n",
      "  winter vv\n",
      "  spring vh\n",
      "  spring vv\n",
      "  summer vh\n",
      "  summer vv\n",
      "  autumn vh\n",
      "  autumn vv\n",
      "polygon mask shape: (10979, 10980)\n",
      "sum polygon_mask_int: 1861770\n",
      "stack and save arrays...\n",
      "Souleimaniye\n",
      "save labels...\n",
      "takes 2 mins\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_val = ld.paths_in_dict(path_data_local_val)\n",
    "\n",
    "file_hfd5_val_X = join(dir_temp_val, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_val_Y = join(dir_temp_val, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_val, file_hfd5_val_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_val, file_hfd5_val_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dict_paths_test = ld.paths_in_dict(path_data_local_test)\n",
    "\n",
    "file_hfd5_test_X = join(dir_temp_test, 'MosNajNaS_20191220_X_{}.hdf5'.format(size_sub_sample[0]))\n",
    "file_hfd5_test_Y = join(dir_temp_test, 'MosNajNaS_20191220_Y_{}.hdf5'.format(size_sub_sample[0]))\n",
    "\n",
    "pp.preprocces_data(dict_paths_test, file_hfd5_test_X, quantile_clip_max, size_sub_sample, dtype, dir_temp_plots)\n",
    "pp.preprocess_shape(dict_paths_test, file_hfd5_test_Y, sub_sample_shape)\n",
    "\n",
    "print('takes {} mins'.format(round((time.time() - t0)/60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and labels are in .npy / .h5 file shape now, but they need to be converted to suitable input for a network.\n",
    "Therefore a full image is cut into many sub-samples of size 'size_sub_sample', e.g. (512, 512). This requires first zero padding (func zero_padding_1) in order to fit the image to an integer times size_sub_sample. <br>\n",
    "Then we augment data by using a stride of size_sub_sample/inv_stride, i.e. we shift the image by for example half a sample size (inv_stride=2). Subsequently we cut into sub-images.  This results in inv_stride<sup>2</sup> times more data. In order to fit n sub-samples into a shifted image, new padding is required (func zero_padding_2). <br>\n",
    "Output is saved in sub dir 'split'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_all(name):\n",
    "#     print(name)\n",
    "\n",
    "# def get_shape(ds):\n",
    "#     print(ds.shape)\n",
    "    \n",
    "# with h5py.File(path_hd5, 'r') as f:\n",
    "#     for key in f.keys():\n",
    "#         ds = f[key]\n",
    "#         print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    #print('padding 2:')\n",
    "    #print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    #print(\" new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices of shape (nrows, ncols)\"\"\"\n",
    "    r, h = array.shape\n",
    "    return (array.reshape(h//nrows, nrows, -1, ncols)\n",
    "                 .swapaxes(1, 2)\n",
    "                 .reshape(-1, nrows, ncols))\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])\n",
    "\n",
    "def list_stride(mode):\n",
    "    if mode == 0:\n",
    "        return ['_']\n",
    "    if mode == 1:\n",
    "        return ['_', 'x', 'y', 'xy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date => 1576842805.787124\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_arr = np.array([1,1])\n",
    "\n",
    "with h5py.File('test.hdf5', 'w') as f:\n",
    "    g = f.create_group('Base_Group')\n",
    "    d = g.create_dataset('default', data=test_arr)\n",
    "\n",
    "    metadata = {'Date': time.time()}\n",
    "\n",
    "    f.attrs.update(metadata)\n",
    "\n",
    "    for m in f.attrs.keys():\n",
    "        print('{} => {}'.format(m, f.attrs[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img=8):\n",
    "    t0 = time.time()\n",
    "    with h5py.File(file_split_hfd5, 'w') as f2:\n",
    "        with h5py.File(file_hfd5, 'r') as f1:\n",
    "\n",
    "            idx_i = 0\n",
    "            meta_data = dict()\n",
    "\n",
    "            for area in sorted(f1.keys()):\n",
    "                print(area)\n",
    "                dset = f1[area]\n",
    "                print(dset.shape)\n",
    "\n",
    "                for dim_str in list_stride(stride_mode):\n",
    "                    print(\"dim_str:\", dim_str)\n",
    "                    np_arr_shape = list(dset.shape)\n",
    "                    if 'x' in dim_str:\n",
    "                        np_arr_shape[0] = np_arr_shape[0] + size_sub_sample[0]\n",
    "                    if 'y' in dim_str:\n",
    "                        np_arr_shape[1] = np_arr_shape[1] + size_sub_sample[1]\n",
    "\n",
    "                    idx_j = int(np_arr_shape[0] * np_arr_shape[1] / (size_sub_sample[0] * size_sub_sample[1])) + idx_i\n",
    "\n",
    "                    if dataset_kind == 'labels':\n",
    "                        np_arr_shape_split = (idx_j,) + size_sub_sample\n",
    "                        maxshape = (None,) + size_sub_sample\n",
    "                    elif dataset_kind == 'data':\n",
    "                        np_arr_shape_split = (idx_j,) + size_sub_sample + (depth_img,)\n",
    "                        maxshape = (None,) + size_sub_sample + (depth_img,)\n",
    "\n",
    "                    if idx_i == 0:\n",
    "                        dset2 = f2.create_dataset(name_dset, \n",
    "                                                    np_arr_shape_split,\n",
    "                                                    dtype=dtype,\n",
    "                                                    maxshape=maxshape,\n",
    "                                                    chunks=True)\n",
    "\n",
    "                    if dataset_kind == 'labels':\n",
    "                        dset2.resize((idx_j,) + size_sub_sample)\n",
    "                        dset2[idx_i:idx_j,:,:] = split_array_2D(zero_padding_2(dset[:], \n",
    "                                                                           dim_str,\n",
    "                                                                           inv_stride, \n",
    "                                                                           size_sub_sample), \n",
    "                                                                size_sub_sample[0], \n",
    "                                                                size_sub_sample[1])\n",
    "                    elif dataset_kind == 'data':\n",
    "                        for k in range(depth_img):\n",
    "                            dset2.resize((idx_j,) + size_sub_sample + (depth_img,))\n",
    "                            dset2[idx_i:idx_j,:,:,k] = split_array_2D(zero_padding_2(dset[:,:,k], \n",
    "                                                                           dim_str,\n",
    "                                                                           inv_stride, \n",
    "                                                                           size_sub_sample), \n",
    "                                                                        size_sub_sample[0], \n",
    "                                                                        size_sub_sample[1])\n",
    "                    print(\"dset2.shape:\", dset2.shape)\n",
    "                    print('idx i,j:', idx_i, idx_j)\n",
    "                    meta_data[area + ' ' + dim_str] = (idx_i, idx_j)\n",
    "                    idx_i = idx_j\n",
    "\n",
    "                f2.attrs.update(meta_data)\n",
    "\n",
    "    print('takes {} mins'.format(round((time.time() - t0) / 60)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (484, 512, 512, 8)\n",
      "idx i,j: 0 484\n",
      "dim_str: x\n",
      "dset2.shape: (990, 512, 512, 8)\n",
      "idx i,j: 484 990\n",
      "dim_str: y\n",
      "dset2.shape: (1496, 512, 512, 8)\n",
      "idx i,j: 990 1496\n",
      "dim_str: xy\n",
      "dset2.shape: (2025, 512, 512, 8)\n",
      "idx i,j: 1496 2025\n",
      "Najaf\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (2509, 512, 512, 8)\n",
      "idx i,j: 2025 2509\n",
      "dim_str: x\n",
      "dset2.shape: (3015, 512, 512, 8)\n",
      "idx i,j: 2509 3015\n",
      "dim_str: y\n",
      "dset2.shape: (3521, 512, 512, 8)\n",
      "idx i,j: 3015 3521\n",
      "dim_str: xy\n",
      "dset2.shape: (4050, 512, 512, 8)\n",
      "idx i,j: 3521 4050\n",
      "Nasiryah\n",
      "(11264, 11264, 8)\n",
      "dim_str: _\n",
      "dset2.shape: (4534, 512, 512, 8)\n",
      "idx i,j: 4050 4534\n",
      "dim_str: x\n",
      "dset2.shape: (5040, 512, 512, 8)\n",
      "idx i,j: 4534 5040\n",
      "dim_str: y\n",
      "dset2.shape: (5546, 512, 512, 8)\n",
      "idx i,j: 5040 5546\n",
      "dim_str: xy\n",
      "dset2.shape: (6075, 512, 512, 8)\n",
      "idx i,j: 5546 6075\n",
      "takes 72 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "\n",
    "prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (484, 512, 512)\n",
      "idx i,j: 0 484\n",
      "dim_str: x\n",
      "dset2.shape: (990, 512, 512)\n",
      "idx i,j: 484 990\n",
      "dim_str: y\n",
      "dset2.shape: (1496, 512, 512)\n",
      "idx i,j: 990 1496\n",
      "dim_str: xy\n",
      "dset2.shape: (2025, 512, 512)\n",
      "idx i,j: 1496 2025\n",
      "Najaf\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (2509, 512, 512)\n",
      "idx i,j: 2025 2509\n",
      "dim_str: x\n",
      "dset2.shape: (3015, 512, 512)\n",
      "idx i,j: 2509 3015\n",
      "dim_str: y\n",
      "dset2.shape: (3521, 512, 512)\n",
      "idx i,j: 3015 3521\n",
      "dim_str: xy\n",
      "dset2.shape: (4050, 512, 512)\n",
      "idx i,j: 3521 4050\n",
      "Nasiryah\n",
      "(11264, 11264)\n",
      "dim_str: _\n",
      "dset2.shape: (4534, 512, 512)\n",
      "idx i,j: 4050 4534\n",
      "dim_str: x\n",
      "dset2.shape: (5040, 512, 512)\n",
      "idx i,j: 4534 5040\n",
      "dim_str: y\n",
      "dset2.shape: (5546, 512, 512)\n",
      "idx i,j: 5040 5546\n",
      "dim_str: xy\n",
      "dset2.shape: (6075, 512, 512)\n",
      "idx i,j: 5546 6075\n",
      "takes 3 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# file_hfd5_labels_split =  join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "\n",
    "# name_dset = 'all_train_data_Y'\n",
    "\n",
    "# with h5py.File(file_hfd5_labels_split, 'w') as f2:\n",
    "#     with h5py.File(file_hfd5_labels, 'r') as f1:\n",
    "#         idx_i = 0\n",
    "#         meta_data = dict()\n",
    "        \n",
    "#         for area in f1.keys():\n",
    "#             print(area)\n",
    "#             dset = f1[area]\n",
    "#             print(dset.shape)\n",
    "\n",
    "#             for dim_str in ['_', 'x', 'y', 'xy']:\n",
    "#                 print(\"dim_str:\", dim_str)\n",
    "#                 np_arr_shape = list(dset.shape)\n",
    "#                 if 'x' in dim_str:\n",
    "#                     np_arr_shape[0] = np_arr_shape[0] + size_sub_sample[0]\n",
    "#                 if 'y' in dim_str:\n",
    "#                     np_arr_shape[1] = np_arr_shape[1] + size_sub_sample[1]\n",
    "                \n",
    "#                 idx_j = int(np_arr_shape[0] * np_arr_shape[1] / (size_sub_sample[0] * size_sub_sample[1])) + idx_i\n",
    "#                 np_arr_shape_split = (idx_j,) + size_sub_sample\n",
    "                \n",
    "#                 if idx_i == 0:\n",
    "#                     dset2 = f2.create_dataset(name_dset, \n",
    "#                                                 np_arr_shape_split,\n",
    "#                                                 dtype=dtype,\n",
    "#                                                 chunks=True)\n",
    "                \n",
    "#                 print(\"dset2.shape:\", dset2.shape)\n",
    "\n",
    "#                 dset2[idx_i:idx_j,:,:] = split_array_2D(zero_padding_2(dset[:], \n",
    "#                                                                        dim_str,\n",
    "#                                                                        inv_stride, \n",
    "#                                                                        size_sub_sample), \n",
    "#                                                         size_sub_sample[0], \n",
    "#                                                         size_sub_sample[1])\n",
    "                \n",
    "                \n",
    "#                 print('idx i,j:', idx_i, idx_j)\n",
    "#                 meta_data[area + ' ' + dim_str] = (idx_i, idx_j)\n",
    "#                 idx_i = idx_j\n",
    "\n",
    "#             f2.attrs.update(meta_data)\n",
    "#             print(\"shape dataset:\", dset2.shape)\n",
    "\n",
    "# print(\"total time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul\n",
      "Najaf\n",
      "Nasiryah\n"
     ]
    }
   ],
   "source": [
    "#TODO: in a different separate step, concatenate datasets of areas\n",
    "# see: https://stackoverflow.com/questions/43929420/how-to-concatenate-two-numpy-arrays-in-hdf5-format\n",
    "\n",
    "# same for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir_label_dev = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/data_temp/train/labels'\n",
    "area_file_label_dev = 'Mosul_label.npy'\n",
    "arr_label_dev = np.load(join(dir_label_dev, 'whole', area_file_label_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-696fb436eb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_x\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print(arr.shape)\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fc3d05e335f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdim_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_y_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim_y\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_y_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new size padding 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cc25739f4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdim_x_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minv_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marr_zeros_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_zeros_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "print('x1:', arr.shape)\n",
    "dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "print('x2:', arr.shape)\n",
    "\n",
    "dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),  dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "sys.getsizeof(arr) / 10**9, arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def zero_padding_1(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print(arr.shape)\n",
    "    dim_x = arr.shape[1]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "    dim_y = arr.shape[2]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "    print(\"new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "    dim_x_add = size_sub_sample[0] - shape_x % size_sub_sample[0]\n",
    "    dim_y_add = size_sub_sample[1] - shape_y % size_sub_sample[1]\n",
    "    \n",
    "def pad_zeros_xy(arr, dim_x_add, dim_y_add, both_sides=False):\n",
    "    arr_zeros_stack_x = np.zeros((dim_x_add, shape_y), dtype=np.uint8)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack_x), axis=0)\n",
    "\n",
    "    if both_sides:\n",
    "        arr = np.concatenate((arr_zeros_stack_x, arr), axis=0)\n",
    "        \n",
    "    arr_zeros_stack_y = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint8)\n",
    "    arr =  np.concatenate((arr, arr_zeros_stack_y), axis=1)\n",
    "    \n",
    "    if both_sides:\n",
    "        arr =  np.concatenate((arr_zeros_stack_y, arr), axis=1)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def zero_padding_2(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('padding 2:')\n",
    "    print(' dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            print('x1:', arr.shape)\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "            print('x2:', arr.shape)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add),\n",
    "                                       dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=2)\n",
    "\n",
    "    print(\" new size array:\", arr.shape)\n",
    "    return arr\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    \n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                                     .swapaxes(1, 2)\n",
    "                                     .reshape(-1, nrows, ncols))\n",
    "    arr_4D_resh1 = np.array(list_3D_arrs)\n",
    "    return arr_4D_resh1.reshape(arr_4D_resh1.shape[1], arr_4D_resh1.shape[0], \n",
    "                                arr_4D_resh1.shape[2], arr_4D_resh1.shape[3])\n",
    "\n",
    "def save_array_in_new_sub_dir(arr, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('save_array_in_new_sub_dir')\n",
    "    name_new_dir = 'split'\n",
    "    dir_data_split = join(main_dir, name_new_dir) \n",
    "    ld.make_dir(dir_data_split)\n",
    "    path_file_split = join(dir_data_split, os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str)\n",
    "    np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_array_2D(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices and .\"\"\"\n",
    "    r, h = array.shape\n",
    "    arr_3D = array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
    "    return [arr for arr in arr_3D]\n",
    "\n",
    "\n",
    "def split_array_3D(array, nrows, ncols):\n",
    "    \"\"\"Split a 3D array into sub-arrays.\"\"\"\n",
    "    w, r, h = array.shape\n",
    "    list_3D_arrs = []\n",
    "        \n",
    "    for i in range(w):\n",
    "        arr_2D = array[i, :, :]\n",
    "\n",
    "        list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n",
    "                             .swapaxes(1, 2)\n",
    "                             .reshape(-1, nrows, ncols))\n",
    "    \n",
    "        arr_4D = np.array(list_3D_arrs)\n",
    "        \n",
    "    arr_4D = arr_4D.reshape(arr_4D.shape[1], arr_4D.shape[0], arr_4D.shape[2], arr_4D.shape[3])\n",
    "    return [arr for arr in arr_4D]\n",
    "\n",
    "def save_samples(list_arrs, main_dir, name_new_dir, filename, dim_str):\n",
    "    print('  save samples')\n",
    "    dir_data_split = join(main_dir, name_new_dir)\n",
    "    ld.make_dir(dir_data_split)\n",
    "\n",
    "    str_filename =  os.path.splitext(filename)[0] + '_' + name_new_dir + '_' + dim_str + '_{}' \n",
    "    for i, arr in enumerate(list_arrs):\n",
    "        path_file_split = join(dir_data_split, str_filename.format(i))\n",
    "        np.save(path_file_split, arr)\n",
    "    del arr\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "def zero_padding_1_labels(arr, sub_sample_shape):\n",
    "    \"\"\"make array dimension a multiple of sub_sample_shape\n",
    "    \"\"\"\n",
    "    print('  padding 1')\n",
    "    dim_x = arr.shape[0]\n",
    "    dim_x_add = sub_sample_shape[0] - dim_x % sub_sample_shape[0]\n",
    "    arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "\n",
    "    dim_y = arr.shape[1]\n",
    "    dim_y_add = sub_sample_shape[1] - dim_y % sub_sample_shape[1]\n",
    "    arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "    arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "    print(\"   new size padding 1:\", arr.shape)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def zero_padding_2_labels(arr, dim_str, inv_stride, size_sub_sample):\n",
    "    \"\"\" padding array to fit stride\n",
    "    \"\"\"\n",
    "    print('  padding 2:')\n",
    "    print('    dim_str:', dim_str)\n",
    "    if dim_str:\n",
    "        if 'x' in dim_str:\n",
    "            dim_x_add = int(np.ceil(size_sub_sample[0] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((dim_x_add, arr.shape[1]), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=0)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=0)\n",
    "        \n",
    "        if 'y' in dim_str:\n",
    "            dim_y_add = int(np.ceil(size_sub_sample[1] / inv_stride))\n",
    "            arr_zeros_stack = np.zeros((arr.shape[0], dim_y_add), dtype=np.uint16)\n",
    "            arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "            arr = np.concatenate((arr_zeros_stack, arr), axis=1)\n",
    "\n",
    "    print(\"   new size array:\", arr.shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new size padding 1: (8, 11264, 11264)\n"
     ]
    }
   ],
   "source": [
    "# zero padding 1\n",
    "dim_x = arr.shape[1]\n",
    "dim_x_add = size_sub_sample[0] - dim_x % size_sub_sample[0]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], dim_x_add, arr.shape[2]), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=1)\n",
    "\n",
    "dim_y = arr.shape[2]\n",
    "dim_y_add = size_sub_sample[1] - dim_y % size_sub_sample[1]\n",
    "arr_zeros_stack = np.zeros((arr.shape[0], arr.shape[1], dim_y_add), dtype=np.uint16)\n",
    "arr = np.concatenate((arr, arr_zeros_stack), axis=2)\n",
    "print(\"new size padding 1:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding 2:\n",
      " dim_str: \n",
      " new size array: (8, 11264, 11264)\n",
      "pad 2 4.0600864 (8, 11264, 11264)\n",
      "len list: 484\n",
      "shape arr: (8, 512, 512)\n",
      "  save samples\n",
      "padding 2:\n",
      " dim_str: x\n",
      "x1: (8, 11264, 11264)\n",
      "x2: (8, 11776, 11264)\n",
      " new size array: (8, 11776, 11264)\n",
      "pad 2 4.244635776 (8, 11776, 11264)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e17f249c2db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlist_arrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_array_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_area_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sub_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'len list:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape arr:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a2e7486f08b8>\u001b[0m in \u001b[0;36msplit_array_3D\u001b[0;34m(array, nrows, ncols)\u001b[0m\n\u001b[1;32m     16\u001b[0m         list_3D_arrs.append(arr_2D.reshape(h//nrows, nrows, -1, ncols)\n\u001b[1;32m     17\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                              .reshape(-1, nrows, ncols))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0marr_4D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_3D_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rest of pipeline\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    #zero padding 2\n",
    "    arr_area_2 = zero_padding_2(arr, dim_str, inv_stride, size_sub_sample)\n",
    "    print('pad 2',sys.getsizeof(arr_area_2) / 10**9, arr_area_2.shape)\n",
    "\n",
    "    #split\n",
    "    list_arrs = split_array_3D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    print('len list:', len(list_arrs))\n",
    "    print('shape arr:', list_arrs[0].shape)\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    \n",
    "    save_samples(list_arrs, dir_data_dev, 'split', area_file_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "    \n",
    "del arr\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  padding 1\n",
      "   new size padding 1: (13312, 13312)\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (13312, 13312)\n",
      "len list: 676\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (13824, 13312)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (13312, 13824)\n",
      "len list: 702\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (13824, 13824)\n",
      "len list: 729\n",
      "  save samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'arr_area' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-af5245f8d0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mlist_arrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0marr_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr_area' is not defined"
     ]
    }
   ],
   "source": [
    "arr_label_dev = zero_padding_1_labels(arr_label_dev, size_sub_sample)\n",
    "\n",
    "\n",
    "for dim_str in ['', 'x', 'y', 'xy']:\n",
    "    arr_area_2 = zero_padding_2_labels(arr_label_dev, dim_str, inv_stride, size_sub_sample)\n",
    "    list_arrs = split_array_2D(arr_area_2, size_sub_sample[0], size_sub_sample[1])\n",
    "    del arr_area_2\n",
    "    gc.collect()\n",
    "    print('len list:', len(list_arrs))\n",
    "    save_samples(list_arrs, dir_label_dev, 'split', area_file_label_dev, dim_str)\n",
    "    del list_arrs\n",
    "    gc.collect()\n",
    "del arr_area\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosul.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah.npy\n",
      "  padding 1\n",
      "   new size padding 1: (8, 11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "   dim_str: \n",
      "   new size array: (8, 11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: x\n",
      "   new size array: (8, 11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: y\n",
      "   new size array: (8, 11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "   dim_str: xy\n",
      "   new size array: (8, 11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Mosul_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Najaf_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "Nasiryah_label.npy\n",
      "  padding 1\n",
      "   new size padding 1: (11264, 11264)\n",
      " augmentation\n",
      "  padding 2:\n",
      "    dim_str: \n",
      "   new size array: (11264, 11264)\n",
      "len list: 484\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: x\n",
      "   new size array: (11776, 11264)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: y\n",
      "   new size array: (11264, 11776)\n",
      "len list: 506\n",
      "  save samples\n",
      "  padding 2:\n",
      "    dim_str: xy\n",
      "   new size array: (11776, 11776)\n",
      "len list: 529\n",
      "  save samples\n",
      "takes 20 mins\n"
     ]
    }
   ],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_hfd5, file_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_val_hfd5, file_val_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_Y.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_Y_split.hdf5')\n",
    "stride_mode = 1\n",
    "#name_dset = 'all_train_data_Y'\n",
    "dataset_kind = 'labels'\n",
    "\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_val_fd5, file_val_split_hfd5, stride_mode, name_dset, dataset_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#file_hfd5 =  join(dir_temp_dev,  'testrun_X.hdf5')\n",
    "#file_split_hfd5 = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "depth_img = 8\n",
    "stride_mode = 1\n",
    "#name_dset = 'all_train_data_X'\n",
    "dataset_kind = 'data'\n",
    "\n",
    "pfn.prepare_data_for_nn_pipeline(file_test_hfd5, file_test_split_hfd5, stride_mode, name_dset, dataset_kind, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# no labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3037, 3037)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__(), len(my_training_batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6075, 512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_hdf5_X, 'r') as f1_Y:\n",
    "    dataset_Y  = f1_Y['all_train_data_X']\n",
    "    print(dataset_Y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H = model.fit(dataset_X, dataset_Y,  \n",
    "                 batch_size=BS, \n",
    "                epochs=EPOCHS, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### load data dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_hdf5_Y, 'r') as f1_Y:\n",
    "    dsety  = f1_Y['all_train_data_Y']\n",
    "    dataset_Y = dsety[:100]\n",
    "    \n",
    "dataset_Y = dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1)#.copy()\n",
    "\n",
    "with h5py.File(file_hdf5_X, 'r') as f1_X:\n",
    "    dsetx = f1_X['all_train_data_X']\n",
    "    dataset_X = dsetx[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 512, 512, 1), (100, 512, 512, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_X_1 = dataset_X[:,:,:,0:1]#.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 81156)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset_X_1), np.sum(dataset_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 512, 512, 1), (32, 512, 512, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.067109008, 0.067109008)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(dataset_X_1) / 10**9, sys.getsizeof(dataset_Y) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  ' channels).')\n",
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "train_datagen_aug = get_augmented(\n",
    "        dataset_X,\n",
    "        dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1),\n",
    "        X_val=None,\n",
    "        Y_val=None,\n",
    "        batch_size=BS,\n",
    "        seed=0,\n",
    "        data_gen_args=dict(\n",
    "            rotation_range=90,\n",
    "            height_shift_range=0,\n",
    "            shear_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir_train_X_split = join(dir_temp_train_X, 'split')\n",
    "dir_train_Y_split = join(dir_temp_train_Y, 'split')\n",
    "\n",
    "dir_val_X_split = join(dir_temp_val_X, 'split')\n",
    "dir_val_Y_split = join(dir_temp_val_Y, 'split')\n",
    "\n",
    "dir_test_X_split = join(dir_temp_test_X, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, dir_X, dir_Y, batch_size):\n",
    "        self.list_files_X = [join(dir_X, file_name) for file_name in os.listdir(dir_X)]\n",
    "        self.list_files_Y = [join(dir_Y, file_name) for file_name in os.listdir(dir_Y)]\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.list_files_X) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_in = self.list_files_X[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y_in = self.list_files_Y[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        b_x = np.array([np.load(file_name) for file_name in batch_x_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        b_y = np.array([np.load(file_name) for file_name in batch_y_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        \n",
    "        return (b_x.reshape(b_x.shape[0], b_x.shape[2], b_x.shape[3], b_x.shape[1]), \n",
    "                b_y.reshape(b_y.shape[0], b_y.shape[1], b_y.shape[2], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(dir_train_X_split, dir_train_Y_split, BS)\n",
    "my_validation_batch_gen = My_Custom_Generator(dir_val_X_split, dir_val_Y_split, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__() * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 512, 512, 8) (31, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "filepath =join(path_model, \"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\")\n",
    "\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "#cb_tb = keras_cb.tensorboard_v1.TensorBoard(log_dir=join(path_model, 'logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            #write_graph=True, write_grads=True, write_images=True, update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(my_training_batch_gen.list_files_X) // BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=len(my_training_batch_gen.list_files_X) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2,\n",
    "#                         validation_data = my_validation_batch_gen,\n",
    "#                         validation_steps = len(my_validation_batch_gen.list_files_X) // BS\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(join(path_model, \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_datagen = get_augmented(\n",
    "#         trainX,\n",
    "#         trainY,\n",
    "#         X_val=None,\n",
    "#         Y_val=None,\n",
    "#         batch_size=BS,\n",
    "#         seed=0,\n",
    "#         data_gen_args=dict(\n",
    "#             rotation_range=90,\n",
    "#             height_shift_range=0,\n",
    "#             shear_range=0,\n",
    "#             horizontal_flip=True,\n",
    "#             vertical_flip=True,\n",
    "#             fill_mode='constant'\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "\n",
    "BS = 2\n",
    "EPOCHS = 2\n",
    "size_sub_sample = (512, 512)\n",
    "\n",
    "inv_stride = 2\n",
    "INIT_LR = 1e-3\n",
    "depth_img = 8\n",
    "\n",
    "IMAGE_DIMS = size_sub_sample + (depth_img,)\n",
    "model_init = satellite_unet(IMAGE_DIMS, num_layers=4)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "print(IMAGE_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "3037 3037\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>1:\n",
    "        break\n",
    "        \n",
    "print(my_training_batch_gen.__len__(), len(my_training_batch_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_epoch(filepath_chk):\n",
    "    return int(filepath_chk.split('-')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_epoch: 0\n"
     ]
    }
   ],
   "source": [
    "#checkpoints\n",
    "filepath_chk = None\n",
    "for i, file in enumerate(os.listdir(path_checkpoints)): \n",
    "    if i == len(os.listdir(path_checkpoints)) - 1:\n",
    "        if file != '.DS_Store':\n",
    "            filepath_chk = join(path_checkpoints, file)\n",
    "            print('checkpoint use: {}'.format(filepath_chk))\n",
    "            \n",
    "            \n",
    "if filepath_chk is not None:\n",
    "    \n",
    "    # Load model:\n",
    "    print('loading model')\n",
    "    model = load_model(filepath_chk)\n",
    "    \n",
    "    # Finding the epoch index from which we are resuming\n",
    "    print('find epoch')\n",
    "    initial_epoch = get_init_epoch(filepath_chk)\n",
    "    \n",
    "#     # Calculating the correct value of count\n",
    "#     count = initial_epoch*batches_per_epoch_test\n",
    "#     # Update the value of count in callback instance\n",
    "#     callbacks_list[1].count = count\n",
    "\n",
    " \n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-2)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    \n",
    "    \n",
    "else:\n",
    "#   model = model_init\n",
    "    filepath_chk = join(path_checkpoints, \"weights-improvement-{epoch:02d}-{iou:.3f}.hdf5\")\n",
    "    initial_epoch = 0\n",
    "    model = model_init\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])\n",
    "    \n",
    "print('initial_epoch:', initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath_chk, monitor='iou', verbose=1, save_best_only=True, \n",
    "                                  mode='max', save_weights_only=False)\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "cb_tb = keras_cb.TensorBoard(log_dir=join(path_model, 'tensorboard_logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            write_graph=True, write_grads=True, write_images=True, \n",
    "                                            update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]# cb_tb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=batches_per_epoch_test,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=1,\n",
    "                        #use_multiprocessing=True,\n",
    "                        #workers=6, \n",
    "                        #max_queue_size=32,\n",
    "                        initial_epoch=initial_epoch\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now()))\n",
    "with open(path_json_file, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "file_plot_train_info = join(path_model, 'info_train_{}.png'.format(utl.datetime_now()))\n",
    "plt.savefig(file_plot_train_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_unosatenv]",
   "language": "python",
   "name": "conda-env-conda_unosatenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
