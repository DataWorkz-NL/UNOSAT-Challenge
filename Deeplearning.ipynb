{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install pandas\n",
    "    !pip install matplotlib\n",
    "    !pip install rasterio\n",
    "    !pip install shapely\n",
    "    !pip install descartes\n",
    "    !pip install pyproj\n",
    "    !pip install geopandas\n",
    "    !pip install keras_unet\n",
    "    !pip install h5py\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    js = ('<script>function ConnectButton(){ '\n",
    "            'console.log(\"Connect pushed\"); '\n",
    "            'document.querySelector(\"#connect\").click()} '\n",
    "            'setInterval(ConnectButton,3000);</script>')\n",
    "    display(HTML(js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/bin/python\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'colab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-56946e20af49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/app_UNOSAT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#sys.path.append(\"/content/drive/My Drive/app_UNOSAT/src\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'colab' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from rasterio.mask import mask\n",
    "from os.path import join\n",
    "import descartes\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import fiona \n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras import callbacks as keras_cb \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# see: https://github.com/karolzak/keras-unet\n",
    "from keras_unet.models import satellite_unet \n",
    "from keras_unet.losses import jaccard_distance\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.utils import get_augmented\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# # prevent Tensorflow memory leakage\n",
    "# K.clear_session()\n",
    "\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "print(sys.executable)\n",
    "\n",
    "if colab:\n",
    "    sys.path.append(\"/content/drive/My Drive/app_UNOSAT\")\n",
    "    #sys.path.append(\"/content/drive/My Drive/app_UNOSAT/src\")\n",
    "\n",
    "from src import load_data as ld\n",
    "from src import preprocessing as pp\n",
    "from src import prepare_for_network as pfn\n",
    "from src import utils as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths, set your own path here\n",
    "if colab:\n",
    "    #path_main = '/content/drive/My Drive/app_UNOSAT'\n",
    "    path_main = '/content/drive/Computers/My Macbook Pro'\n",
    "else:\n",
    "    path_main = '/Volumes/other/datasets_and_ML/UNOSAT_Challenge'\n",
    "\n",
    "path_data_main = utl.make_dir(join(path_main, \"data\"))\n",
    "\n",
    "#### put data in these dirs\n",
    "\n",
    "# I choose:\n",
    "# Training: Mosul, Najaf, Nasiryah\n",
    "# Validation: Souleimaniye\n",
    "# Test: Bagdad, Kirkouk, Samawah, Tikrit (no labels present)\n",
    "path_data_local_train = join(path_data_main, 'Train_Dataset')\n",
    "path_data_local_val = join(path_data_main, 'Validation_Dataset')\n",
    "path_data_local_test = join(path_data_main, 'Evaluation_Dataset')\n",
    "\n",
    "path_model =  utl.make_dir(join(path_data_main, 'model'))\n",
    "\n",
    "# other dirs are made automatically\n",
    "dir_temp_data =  utl.make_dir(join(path_data_main, 'data_temp'))\n",
    "dir_temp_train = utl.make_dir(join(dir_temp_data, 'train'))\n",
    "dir_temp_val = utl.make_dir(join(dir_temp_data, 'val'))\n",
    "dir_temp_dev = utl.make_dir(join(dir_temp_data, 'data_dev'))\n",
    "dir_temp_eval = utl.make_dir(join(dir_temp_data, 'evaluation'))\n",
    "dir_temp_plots = utl.make_dir(join(dir_temp_data, 'plots_preprocessing'))\n",
    "\n",
    "# settings preprocessing\n",
    "quantile_clip_max = 0.999\n",
    "size_sub_sample = (512, 512)\n",
    "dtype = 'i16'\n",
    "inv_stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### develop (don't run for pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3037, 3037)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__(), len(my_training_batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6075, 512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_hdf5_X, 'r') as f1_Y:\n",
    "    dataset_Y  = f1_Y['all_train_data_X']\n",
    "    print(dataset_Y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H = model.fit(dataset_X, dataset_Y,  \n",
    "                 batch_size=BS, \n",
    "                epochs=EPOCHS, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(file_hdf5_Y, 'r') as f1_Y:\n",
    "    dsety  = f1_Y['all_train_data_Y']\n",
    "    dataset_Y = dsety[:100]\n",
    "    \n",
    "dataset_Y = dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1)#.copy()\n",
    "\n",
    "with h5py.File(file_hdf5_X, 'r') as f1_X:\n",
    "    dsetx = f1_X['all_train_data_X']\n",
    "    dataset_X = dsetx[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 512, 512, 1), (100, 512, 512, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_X_1 = dataset_X[:,:,:,0:1]#.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 81156)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset_X_1), np.sum(dataset_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 512, 512, 1), (32, 512, 512, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Y.shape, dataset_X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.067109008, 0.067109008)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(dataset_X_1) / 10**9, sys.getsizeof(dataset_Y) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  ' channels).')\n",
      "/Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (100, 512, 512, 8) (8 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "train_datagen_aug = get_augmented(\n",
    "        dataset_X,\n",
    "        dataset_Y.reshape(dataset_Y.shape[0], dataset_Y.shape[1], dataset_Y.shape[2], 1),\n",
    "        X_val=None,\n",
    "        Y_val=None,\n",
    "        batch_size=BS,\n",
    "        seed=0,\n",
    "        data_gen_args=dict(\n",
    "            rotation_range=90,\n",
    "            height_shift_range=0,\n",
    "            shear_range=0,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dir_train_X_split = join(dir_temp_train_X, 'split')\n",
    "dir_train_Y_split = join(dir_temp_train_Y, 'split')\n",
    "\n",
    "dir_val_X_split = join(dir_temp_val_X, 'split')\n",
    "dir_val_Y_split = join(dir_temp_val_Y, 'split')\n",
    "\n",
    "dir_test_X_split = join(dir_temp_test_X, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, dir_X, dir_Y, batch_size):\n",
    "        self.list_files_X = [join(dir_X, file_name) for file_name in os.listdir(dir_X)]\n",
    "        self.list_files_Y = [join(dir_Y, file_name) for file_name in os.listdir(dir_Y)]\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.list_files_X) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_in = self.list_files_X[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y_in = self.list_files_Y[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        b_x = np.array([np.load(file_name) for file_name in batch_x_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        b_y = np.array([np.load(file_name) for file_name in batch_y_in if os.path.splitext(file_name)[1] == '.npy'])\n",
    "        \n",
    "        return (b_x.reshape(b_x.shape[0], b_x.shape[2], b_x.shape[3], b_x.shape[1]), \n",
    "                b_y.reshape(b_y.shape[0], b_y.shape[1], b_y.shape[2], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(dir_train_X_split, dir_train_Y_split, BS)\n",
    "my_validation_batch_gen = My_Custom_Generator(dir_val_X_split, dir_val_Y_split, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_batch_gen.__len__() * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 512, 512, 8) (31, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n",
      "(32, 512, 512, 8) (32, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "filepath =join(path_model, \"weights-improvement-{epoch:02d}-{iou:.2f}.hdf5\")\n",
    "\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "#cb_tb = keras_cb.tensorboard_v1.TensorBoard(log_dir=join(path_model, 'logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            #write_graph=True, write_grads=True, write_images=True, update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(my_training_batch_gen.list_files_X) // BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jurriaan/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=len(my_training_batch_gen.list_files_X) // BS,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=2,\n",
    "#                         validation_data = my_validation_batch_gen,\n",
    "#                         validation_steps = len(my_validation_batch_gen.list_files_X) // BS\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(join(path_model, \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_datagen = get_augmented(\n",
    "#         trainX,\n",
    "#         trainY,\n",
    "#         X_val=None,\n",
    "#         Y_val=None,\n",
    "#         batch_size=BS,\n",
    "#         seed=0,\n",
    "#         data_gen_args=dict(\n",
    "#             rotation_range=90,\n",
    "#             height_shift_range=0,\n",
    "#             shear_range=0,\n",
    "#             horizontal_flip=True,\n",
    "#             vertical_flip=True,\n",
    "#             fill_mode='constant'\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "\n",
    "BS = 2\n",
    "EPOCHS = 2\n",
    "size_sub_sample = (512, 512)\n",
    "\n",
    "inv_stride = 2\n",
    "INIT_LR = 1e-3\n",
    "depth_img = 8\n",
    "\n",
    "IMAGE_DIMS = size_sub_sample + (depth_img,)\n",
    "model = satellite_unet(IMAGE_DIMS, num_layers=4)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "print(IMAGE_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(Sequence):\n",
    "    def __init__(self, file_hdf5_X, file_hdf5_Y, batch_size):\n",
    "        self.file_hdf5_X = file_hdf5_X\n",
    "        self.file_hdf5_Y = file_hdf5_Y\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def __len__(self) :\n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            len_dataset_Y = dataset_Y.shape[0]\n",
    "        \n",
    "        return (np.ceil(len_dataset_Y) / float(self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_hdf5_X, 'r') as f1_X:\n",
    "            dataset_X  = f1_X['all_train_data_X']\n",
    "            batch_x = dataset_X[idx * self.batch_size : (idx+1) * self.batch_size, :, :, :]\n",
    "        \n",
    "        with h5py.File(self.file_hdf5_Y, 'r') as f1_Y:\n",
    "            dataset_Y  = f1_Y['all_train_data_Y']\n",
    "            batch_y = dataset_Y[idx * self.batch_size : (idx+1) * self.batch_size, :, :]\n",
    "\n",
    "        return (batch_x, batch_y.reshape(batch_y.shape[0], batch_y.shape[1], batch_y.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_hdf5_X = join(dir_temp_dev,  'testrun_X_split.hdf5')\n",
    "file_hdf5_Y = join(dir_temp_dev,  'testrun_Y_split.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_gen = My_Custom_Generator(file_hdf5_X, file_hdf5_Y, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "(2, 512, 512, 8) (2, 512, 512, 1)\n",
      "3037 3037\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(my_training_batch_gen):\n",
    "    print(arr[0].shape, arr[1].shape)\n",
    "    if i>1:\n",
    "        break\n",
    "        \n",
    "print(my_training_batch_gen.__len__(), len(my_training_batch_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  # metrics=[\"accuracy\"],\n",
    "                  metrics=[iou, iou_thresholded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint:\n",
    "if checkpoint_path is not None:\n",
    "    # Load model:\n",
    "    model = load_model(checkpoint_path)\n",
    "    # Finding the epoch index from which we are resuming\n",
    "    initial_epoch = get_init_epoch(checkpoint_path)\n",
    "else:\n",
    "    model = build_model_func()\n",
    "    initial_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint use: /Volumes/other/datasets_and_ML/UNOSAT_Challenge/data/model/weights-improvement-02-0.05.hdf5\n"
     ]
    }
   ],
   "source": [
    "#checkpoints\n",
    "filepath_chk = None\n",
    "for file in os.listdir(path_model):\n",
    "    filepath_chk = join(path_model, file)\n",
    "if not filepath_chk:\n",
    "    filepath_chk = join(path_model, \"weights-improvement-{epoch:02d}-{iou:.3f}.hdf5\")\n",
    "    \n",
    "print('checkpoint use: {}'.format(filepath_chk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "cb_chk = keras_cb.ModelCheckpoint(filepath_chk, monitor='iou', verbose=1, save_best_only=True, mode='max')\n",
    "#cb_proglog = keras_cb.ProgbarLogger()\n",
    "cb_es = keras_cb.EarlyStopping(patience=5)\n",
    "cb_tb = keras_cb.TensorBoard(log_dir=join(path_model, 'tensoboard_logs'), histogram_freq=1, batch_size=BS, \n",
    "                                            write_graph=True, write_grads=True, write_images=True, \n",
    "                                            update_freq='epoch')\n",
    "callbacks_list = [cb_chk, cb_es]# cb_tb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   9/3037 [..............................] - ETA: 25:39:21 - loss: 0.4825 - iou: 0.0112 - iou_thresholded: 0.0176"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c0f789ea4e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0;31m#use_multiprocessing=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0;31m#workers=6,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda_unosatenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=my_training_batch_gen,\n",
    "                        steps_per_epoch=len(my_training_batch_gen),\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=1,\n",
    "                        #use_multiprocessing=True,\n",
    "                        #workers=6, \n",
    "                        #max_queue_size=32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "path_json_file = join(path_model, \"model_{}.json\".format(utl.datetime_now), \"w\") \n",
    "with open(path_json_file) as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(join(path_model, \"model_epochs_{}.h5\".format(utl.datetime_now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"iou\"], label=\"train_iou\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/iou\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "=\n",
    "file_plot_train_info = join(path_model, 'info_train_{}.png'.format(utl.datetime_now()))\n",
    "plt.savefig(file_plot_train_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_unosatenv]",
   "language": "python",
   "name": "conda-env-conda_unosatenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
